{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fall 2020 CX4641/CS7641 A Homework 2\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Oct 6th, Tuesday, 11:59 pm AOE\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- Q4 is bonus for both undergraduate and graduate students.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You can directly type Latex equations into markdown cells.\n",
    "\n",
    "- Typing with Latex\\markdown is required for all the written questions. Handwritten answers will not be accepted. \n",
    "    \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "\n",
    "## Using the autograder\n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW2: \"HW2 - Programming\" and \"HW2 - Non-programming\".\n",
    "\n",
    "- You will submit your code for the autograder on \"HW2 - Programming\" in the following format:\n",
    "\n",
    "    * kmeans.py\n",
    "    * gmm.py\n",
    "    * semisupervised.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes \"Kmeans\", \"GMM\", \"CleanData\", \"SemiSupervised\" onto the respective files. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "\n",
    "- **For the \"HW2 - Non-programming\" part, you will download your jupyter notbook as html and submit it as a PDF on Gradescope. To download the notebook as PDF, click on \"File\" on the top left corner of this page and select \"Download as > PDF\". The non-programming part corresponds to Q2, Q3.3 (both your response and the generated images with your implementation) and Q4.2**\n",
    "- **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBMpzE5S68ie"
   },
   "source": [
    "## 0 Set up\n",
    "This notebook is tested under [python 3.\\*.\\*](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "Please implement the functions that have \"raise NotImplementedError\", and after you finish the coding, please delete or comment \"raise NotImplementedError\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n",
      "matplotlib: 3.2.2\n",
      "numpy: 1.18.5\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import sys\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "# Set random seed so output is all same\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KMeans Clustering [5 + 30 + 10 + 5 + 10 pts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KMeans is trying to solve the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\min_S \\sum_{i=1}^K \\sum_{x_j \\in S_i} ||x_j - \\mu_i||^2\n",
    "\\end{align}\n",
    "where one needs to partition the N observations into K clusters: $S = \\{S_1, S_2, \\ldots, S_K\\}$ and each cluster has $\\mu_i$ as its center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 pairwise distance [5pts]\n",
    "\n",
    "In this section, you are asked to implement pairwise_dist function.\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{N x D}$ and $Y \\in \\mathbb{R}^{M x D}$, obtain the pairwise distance matrix $dist \\in \\mathbb{R}^{N x M}$ using the euclidean distance metric, where $dist_{i, j} = ||X_i - Y_j||_2$.  \n",
    "\n",
    "DO NOT USE FOR LOOP in your implementation -- they are slow and will make your code too slow to pass our grader.  Use array broadcasting instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 KMeans Implementation [30pts]\n",
    "\n",
    "In this section, you are asked to implement _init_centers [5pts], _update_assignment [10pts], _update_centers [10pts] and _get_loss function [5pts].\n",
    "\n",
    "For the function signature, please see the corresponding doc strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Find the optimal number of clusters [10 pts]\n",
    "\n",
    "In this section, you are asked to implement find_optimal_num_clusters function.\n",
    "\n",
    "You will now use the elbow method to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Autograder test to find centers for data points [5 pts]\n",
    "\n",
    "To obtain these 5 points, you need to be pass the tests set up in the autograder. These will test the centers created by your implementation. Be sure to upload the correct files to obtain these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \n",
    "    def __init__(self): #No need to implement\n",
    "        pass\n",
    "    \n",
    "    def pairwise_dist(self, x, y): # [5 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "            y: M x D numpy array\n",
    "        Return:\n",
    "                dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
    "                x[i, :] and y[j, :]\n",
    "                \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        N, D = np.shape(x)\n",
    "        reshaped_x = np.reshape(x, (N, 1, D))\n",
    "        euclidean = np.sqrt(np.sum(np.square(reshaped_x - y), axis=2))\n",
    "        return euclidean\n",
    "\n",
    "    def _init_centers(self, points, K, **kwargs): # [5 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            centers: K x D numpy array, the centers. \n",
    "        \"\"\"\n",
    "        # raise NotImplementedError\n",
    "        _, D = np.shape(points)\n",
    "        options = points.flatten()\n",
    "        centersArray = np.random.choice(options, (K, D))\n",
    "        return centersArray\n",
    "\n",
    "    def _update_assignment(self, centers, points): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            \n",
    "        Hint: You could call pairwise_dist() function.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_centers(self, old_centers, cluster_idx, points): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_loss(self, centers, cluster_idx, points): # [5 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            loss: a single float number, which is the objective function of KMeans. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            max_iters: maximum number of iterations (Hint: You could change it when debugging)\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            cluster assignments: Nx1 int numpy array\n",
    "            cluster centers: K x D numpy array, the centers\n",
    "            loss: final loss value of the objective function of KMeans\n",
    "        \"\"\"\n",
    "        centers = self._init_centers(points, K, **kwargs)\n",
    "        for it in range(max_iters):\n",
    "            cluster_idx = self._update_assignment(centers, points)\n",
    "            centers = self._update_centers(centers, cluster_idx, points)\n",
    "            loss = self._get_loss(centers, cluster_idx, points)\n",
    "            K = centers.shape[0]\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    break\n",
    "            prev_loss = loss\n",
    "            if verbose:\n",
    "                print('iter %d, loss: %.4f' % (it, loss))\n",
    "        return cluster_idx, centers, loss\n",
    "    \n",
    "    def find_optimal_num_clusters(self, data, max_K=15): # [10 pts]\n",
    "        \"\"\"Plots loss values for different number of clusters in K-Means\n",
    "        \n",
    "        Args:\n",
    "            image: input image of shape(H, W, 3)\n",
    "            max_K: number of clusters\n",
    "        Return:\n",
    "            losses: an array of loss denoting the loss of each number of clusters\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Expected Answer ***\n",
      "==x==\n",
      "[[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]]\n",
      "==y==\n",
      "[[ 0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038]]\n",
      "==dist==\n",
      "[[1.85239052 0.19195729 1.35467638]\n",
      " [1.85780729 2.29426447 1.18155842]]\n",
      "\n",
      "*** My Answer ***\n",
      "==x==\n",
      "[[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]]\n",
      "==y==\n",
      "[[ 0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038]]\n",
      "==dist==\n",
      "[[1.85239052 0.19195729 1.35467638]\n",
      " [1.85780729 2.29426447 1.18155842]]\n"
     ]
    }
   ],
   "source": [
    "# Helper function for checking the implementation of pairwise_distance fucntion. Please DO NOT change this function\n",
    "# TEST CASE\n",
    "x = np.random.randn(2, 2)\n",
    "y = np.random.randn(3, 2)\n",
    "\n",
    "print(\"*** Expected Answer ***\")\n",
    "print(\"\"\"==x==\n",
    "[[ 1.62434536 -0.61175641]\n",
    " [-0.52817175 -1.07296862]]\n",
    "==y==\n",
    "[[ 0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069 ]\n",
    " [ 0.3190391  -0.24937038]]\n",
    "==dist==\n",
    "[[1.85239052 0.19195729 1.35467638]\n",
    " [1.85780729 2.29426447 1.18155842]]\"\"\")\n",
    "\n",
    "\n",
    "print(\"\\n*** My Answer ***\")\n",
    "print(\"==x==\")\n",
    "print(x)\n",
    "print(\"==y==\")\n",
    "print(y)\n",
    "print(\"==dist==\")\n",
    "print(KMeans().pairwise_dist(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_matrix(image_file, grays=False):\n",
    "    \"\"\"\n",
    "    Convert .png image to matrix\n",
    "    of values.\n",
    "    params:\n",
    "    image_file = str\n",
    "    grays = Boolean\n",
    "    returns:\n",
    "    img = (color) np.ndarray[np.ndarray[np.ndarray[float]]]\n",
    "    or (grayscale) np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    img = plt.imread(image_file)\n",
    "    # in case of transparency values\n",
    "    if len(img.shape) == 3 and img.shape[2] > 3:\n",
    "        height, width, depth = img.shape\n",
    "        new_img = np.zeros([height, width, 3])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r, c, :] = img[r, c, 0:3]\n",
    "        img = np.copy(new_img)\n",
    "    if grays and len(img.shape) == 3:\n",
    "        height, width = img.shape[0:2]\n",
    "        new_img = np.zeros([height, width])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r, c] = img[r, c, 0]\n",
    "        img = new_img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images/bird_color_24.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-4a29786b9224>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mimage_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage_to_matrix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'./images/bird_color_24.png'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage_values\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage_values\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage_values\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-5834a6c819f6>\u001B[0m in \u001B[0;36mimage_to_matrix\u001B[0;34m(image_file, grays)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mgrayscale\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \"\"\"\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0;31m# in case of transparency values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m3\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001B[0m in \u001B[0;36mimread\u001B[0;34m(fname, format)\u001B[0m\n\u001B[1;32m   2059\u001B[0m \u001B[0;34m@\u001B[0m\u001B[0mdocstring\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2060\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2061\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2062\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2063\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001B[0m in \u001B[0;36mimread\u001B[0;34m(fname, format)\u001B[0m\n\u001B[1;32m   1472\u001B[0m             \u001B[0mfd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBytesIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0murlopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1473\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0m_png\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_png\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1474\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen_file_cm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1475\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_png\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_png\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1476\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/contextlib.py\u001B[0m in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[0;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"generator didn't yield\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001B[0m in \u001B[0;36mopen_file_cm\u001B[0;34m(path_or_file, mode, encoding)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mopen_file_cm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_or_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 418\u001B[0;31m     \u001B[0mfh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopened\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mto_filehandle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_or_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    419\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mopened\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    420\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mfh\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001B[0m in \u001B[0;36mto_filehandle\u001B[0;34m(fname, flag, return_opened, encoding)\u001B[0m\n\u001B[1;32m    401\u001B[0m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbz2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBZ2File\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    402\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 403\u001B[0;31m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    404\u001B[0m         \u001B[0mopened\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    405\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'seek'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './images/bird_color_24.png'"
     ]
    }
   ],
   "source": [
    "image_values = image_to_matrix('./images/bird_color_24.png')\n",
    "\n",
    "r = image_values.shape[0]\n",
    "c = image_values.shape[1]\n",
    "ch = image_values.shape[2]\n",
    "# flatten the image_values\n",
    "image_values = image_values.reshape(r*c,ch)\n",
    "\n",
    "k = 6 # feel free to change this value\n",
    "cluster_idx, centers, loss = KMeans()(image_values, k)\n",
    "updated_image_values = np.copy(image_values)\n",
    "\n",
    "# assign each pixel to cluster mean\n",
    "for i in range(0,k):\n",
    "    indices_current_cluster = np.where(cluster_idx == i)[0]\n",
    "    updated_image_values[indices_current_cluster] = centers[i]\n",
    "    \n",
    "updated_image_values = updated_image_values.reshape(r,c,ch)\n",
    "\n",
    "plt.figure(None,figsize=(9,12))\n",
    "plt.imshow(updated_image_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans().find_optimal_num_clusters(image_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Coefficient Evaluation [10 pts]\n",
    "\n",
    "The average silhouette of the data is another useful criterion for assessing the natural number of clusters. The silhouette of a data instance is a measure of how closely it is matched to data within its cluster and how loosely it is matched to data of the neighbouring cluster.\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_cluster_dist(cluster_idx, data, labels): # [4 pts]\n",
    "    \"\"\"\n",
    "    Calculates the average distance from a point to other points within the same cluster\n",
    "    \n",
    "    Args:\n",
    "        cluster_idx: the cluster index (label) for which we want to find the intra cluster distance\n",
    "        data: NxD numpy array, where N is # points and D is the dimensionality\n",
    "        labels: 1D array of length N where each number indicates of cluster assignement for that point\n",
    "    Return:\n",
    "        intra_dist_cluster: 1D array where the i_th entry denotes the average distance from point i \n",
    "                            in cluster denoted by cluster_idx to other points within the same cluster\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def inter_cluster_dist(cluster_idx, data, labels): # [4 pts]\n",
    "    \"\"\"\n",
    "    Calculates the average distance from one cluster to the nearest cluster\n",
    "    Args:\n",
    "        cluster_idx: the cluster index (label) for which we want to find the intra cluster distance\n",
    "        data: NxD numpy array, where N is # points and D is the dimensionality\n",
    "        labels: 1D array of length N where each number indicates of cluster assignement for that point\n",
    "    Return:\n",
    "        inter_dist_cluster: 1D array where the i-th entry denotes the average distance from point i in cluster\n",
    "                            denoted by cluster_idx to the nearest neighboring cluster\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def silhouette_coefficient(data, labels): #[2 pts]\n",
    "    \"\"\"\n",
    "    Finds the silhouette coefficient of the current cluster assignment\n",
    "    \n",
    "    Args:\n",
    "        data: NxD numpy array, where N is # points and D is the dimensionality\n",
    "        labels: 1D array of length N where each number indicates of cluster assignement for that point\n",
    "    Return:\n",
    "        silhouette_coefficient: Silhouette coefficient of the current cluster assignment\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette_coefficient(data, max_K=15):\n",
    "    \"\"\"\n",
    "    Plot silhouette coefficient for different number of clusters, no need to implement\n",
    "    \"\"\"\n",
    "    clusters = np.arange(2, max_K+1)\n",
    "    \n",
    "    silhouette_coefficients = []\n",
    "    for k in range(2, max_K+1):\n",
    "        labels, _, _ = KMeans()(data, k)\n",
    "        silhouette_coefficients.append(silhouette_coefficient(data, labels))\n",
    "    plt.plot(clusters, silhouette_coefficients)\n",
    "    return silhouette_coefficients\n",
    "\n",
    "\n",
    "data = np.random.rand(200,3) * 100\n",
    "plot_silhouette_coefficient(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of K-Means\n",
    "\n",
    "One of the limitations of K-Means Clustering is that it dependes largely on the shape of the dataset. A common example of this is trying to cluster one circle within another (concentric circles). A K-means classifier will fail to do this and will end up effectively drawing a line which crosses the circles. You can visualize this limitation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize limitation of kmeans, do not have to implement\n",
    "from sklearn.datasets.samples_generator import (make_circles, make_moons)\n",
    "\n",
    "X1, y1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n",
    "X2, y2 = make_moons(noise=0.05, n_samples=1500)\n",
    "\n",
    "def visualise(X, C, K):# Visualization of clustering. You don't need to change this function   \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=C,cmap='rainbow')\n",
    "    plt.title('Visualization of K = '+str(K), fontsize=15)\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "cluster_idx1, centers1, loss1 = KMeans()(X1, 2)\n",
    "visualise(X1, cluster_idx1, 2)\n",
    "\n",
    "cluster_idx2, centers2, loss2 = KMeans()(X2, 2)\n",
    "visualise(X2, cluster_idx2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EM algorithm [20 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Performing EM Update [10 pts]\n",
    "\n",
    "A univariate Gaussian Mixture Model (GMM) has two components, both of which have their own mean and standard deviation. The model is defined by the following parameters:\n",
    "\n",
    "$$ \\mathbf{z} \\sim Bernoulli(\\theta) $$\n",
    "$$ \\mathbf{p(x|z=0)} \\sim \\mathcal{N}(\\mu, \\sigma) $$\n",
    "$$ \\mathbf{p(x|z=1)} \\sim \\mathcal{N}(2\\mu, 3\\sigma) $$\n",
    "\n",
    "For a dataset of N datapoints, find the following: \n",
    "\n",
    "\n",
    "2.1.1. Write the marginal probability of x, i.e. $\\mathbf{p(x)}$  \\[2pts] \n",
    "\n",
    "\n",
    "2.1.2. E-Step: Compute the posterior probability, i.e, $p(z^i=k|x^i)$, where k = {0,1} \\[2pts]\n",
    "\n",
    "\n",
    "2.1.3. M-Step: Compute the updated value of $\\mu$ (You can keep $\\sigma$ fixed for this) \\[3pts]\n",
    "\n",
    "\n",
    "2.1.4. M-Step: Compute the updated value for $\\sigma$ (You can keep $\\mu$ fixed for this) \\[3pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EM Algorithm in ABO Blood Groups [10 pts]\n",
    "\n",
    "In the ABO blood group system, each individual has a phenotype and a genotype as shown below. The genotype is made of underlying alleles (A, B, O).\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\\hline Phenotype & Genotype   \\\\ \\hline A & AA   \\\\ \\hline A & AO \n",
    "\\\\ \\hline A & OA  \\\\ \\hline B & BB  \\\\ \\hline B & BO \n",
    "\\\\ \\hline B & OB  \\\\ \\hline O & OO \\\\ \\hline AB & AB\n",
    "\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "In a research experiment, scientists wanted to model the distribution of the genotypes of the population. They collected the phenotype information from the participants as this could be directly observed from the individual's blood group. The scientists, however want to use this data to model the underlying genotype information. In order to help them obtain an understanding, you suggest using the EM algorithm to find out the genotype distribution.\n",
    "\n",
    "You know that the probability of that an allele is present in an individual is independent of the probability of any other allele, i.e, $P(AO) = P(OA) = P(A)* P(O)$ and so on. Also note that the genotype pairs: (AO, OA) and (BO, OB) are identical and can be treated as AO, BO respectively. You also know that the alleles follow a multinomial distribution.\n",
    "$$ p(O) = 1 - p(A) - p(B) $$\n",
    "\n",
    "Let $ n_A, n_B, n_O, n_{AB}$ be the number of individuals with the phenotypes A, B, O and AB respectively.\\\n",
    "Let $ n_{AA}, n_{AO}, n_{BB}, n_{BO}, n_{AB} $ be the numbers of individuals with genotypes AA, AO, BB, BO and AB respectively.\\\n",
    "The satisfy the following conditions: \n",
    "$$ n_A = n_{AA} + n_{AO} $$ $$n_B = n_{BB} + n_{BO} $$\n",
    "$$ n_A + n_B + n_O + n_{AB} = n $$\n",
    "\n",
    "\n",
    "Given:\n",
    "$$ p_A = p_B = p_O = \\frac{1}{3} $$\n",
    "$$ n_A = 186, n_B = 38, n_O = 284, n_{AB} = 13 $$\n",
    "\n",
    "2.2.1. \n",
    "In the E step, compute the value of  $n_{AA}, n_{AO}, n_{BB}, n_{BO} $.  \\[5pts]\n",
    "\n",
    "2.2.2. In the M step, find the new value of $p_A, p_B$ given the updated values from E-step above.\n",
    "(Round off the answer to 3 decimal places)   \\[5pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GMM implementation [40 + 10 + 5(bonus) pts]\n",
    "\n",
    "A Gaussian Mixture Model(GMM) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian Distribution. In a nutshell, GMM is a soft clustering algorithm in a sense that each data point is assigned to a cluster with a probability. In order to do that, we need to convert our clustering problem into an inference problem.\n",
    "\n",
    "Given $N$ samples $X = [x_1, x_2, \\ldots, x_N]^T$, where $x_i \\in \\mathbb{R}^D$. Let $\\pi$ be a K-dimentional probability distribution and $(\\mu_k; \\Sigma_k)$ be the mean and covariance matrix of the $k^{th}$ Gaussian distribution in $\\mathbb{R}^d$. \n",
    "\n",
    "The GMM object implements EM algorithms for fitting the model and MLE for optimizing its parameters. It also has some particular hypothesis on how the data was generated:\n",
    "\n",
    "- Each data point $x_i$ is assigned to a cluster $k$ with probability of $\\pi_k$ where $\\sum_{k=1}^K \\pi_k = 1$\n",
    "- Each data point $x_i$ is generated from Multivariate Normal Distribution $\\cal{N}(\\mu_k, \\Sigma_k)$ where $\\mu_k \\in \\mathbb{R}^D$ and $\\Sigma_k \\in \\mathbb{R}^{D\\times D}$\n",
    "\n",
    "Our goal is to find a $K$-dimension Gaussian distributions to model our data $X$. This can be done by learning the parameters $\\pi, \\mu$ and $\\Sigma$ through likelihood function. Detailed derivation can be found in our slide of GMM. The log-likelihood function now becomes:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ln } p(x_1, \\dots, x_N | \\pi, \\mu, \\Sigma) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture we know that MLEs for GMM all depend on each other and the responsibility $\\tau$. Thus, we need to use an iterative algorithm (the EM algorithm) to find the estimate of parameters that maximize our likelihood function. **All detailed derivations can be found in the lecture slide of GMM.**\n",
    "\n",
    "- **E-step:** Evaluate the responsibilities\n",
    "\n",
    "In this step, we need to calculate the responsibility $\\tau$, which is the conditional probability that a data point belongs to a specific cluster $k$ if we are given the datapoint, i.e. $P(z_k|x)$. The formula for $\\tau$ is given below:\n",
    "\n",
    "$$\n",
    "\\tau\\left(z_k\\right)=\\frac{\\pi_{k} N\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} N\\left(x | \\mu_{j}, \\Sigma_{j}\\right)}, \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "Note that each data point should have one probability for each component/cluster. For this homework, you will work with $\\tau\\left(z_k\\right)$ which has a size of $N\\times K$ and you should have all the responsibility values in one matrix. **We use gamma as $\\tau$ in this homework**.\n",
    "\n",
    "- **M-step:** Re-estimate Paramaters\n",
    "\n",
    "After we obtained the responsibility, we can find the update of parameters, which are given below:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k^{new} &= \\dfrac{\\sum_{n=1}^N \\tau(z_k)x_n}{N_k} \\\\\n",
    "\\Sigma_k^{new} &= \\dfrac{1}{N_k}\\sum_{n=1}^N \\tau (z_k)^T(x_n - \\mu_k^{new})^T(x_n-\\mu_k^{new}) \\\\\n",
    "\\pi_k^{new} &= \\dfrac{N_k}{N}\n",
    "\\end{align}\n",
    "where $N_k = \\sum_{n=1}^N \\tau(z_k)$. Note that the updated value for $\\mu_k$ is used when updating $\\Sigma_k$. The multiplication of $\\tau (z_k)^T(x_n - \\mu_k^{new})^T$ is element-wise so it will preserve the dimensions of $(x_n - \\mu_k^{new})^T$.\n",
    "\n",
    "- We repeat E and M steps until the incremental improvement to the likelihood function is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Notes**\n",
    "- For undergraduate students: you may assume that the covariance matrix $\\Sigma$ is a diagonal matrix, which means the features are independent. (i.e. the red intensity of a pixel is independent of its blue intensity, etc). \n",
    "- For graduate students: please assume a full covariance matrix.\n",
    "- The class notes assume that your dataset $X$ is $(D, N)$. However, the homework dataset is $(N, D)$ as mentioned on the instructions, so the formula is a little different from the lecture note in order to obtain the right dimensions of parameters.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. **DO NOT USE FOR LOOPS OVER N.** You can always find a way to avoid looping over the observation data points in our homework problem. If you have to loop over D or K, that would be fine.\n",
    "\n",
    "2. You can initiate $\\pi(k)$ the same for each $k$, i.e. $\\pi(k) = \\frac{1}{K}, \\forall k = 1, 2, \\ldots, K$.\n",
    "\n",
    "3. In part 3 you are asked to generate the model for pixel clustering of image. We will need to use a multivariate Gaussian because each image will have $N$ pixels and $D=3$ features, which correspond to red, green, and blue color intensities. It means that each image is a $(N\\times3)$ dataset matrix. In the following parts, remember $D=3$ in this problem.\n",
    "\n",
    "4. To avoid using for loops in your code, we recommend you take a look at the concept [Array Broadcasting in Numpy](https://numpy.org/doc/stable/user/theory.broadcasting.html#array-broadcasting-in-numpy). Also, some calculations that required different shapes of arrays can be achieved by broadcasting. \n",
    "\n",
    "5. Be careful of the dimensions of your parameters. Before you test anything on the autograder, please look at the instructions below on the shapes of the variables you need to output. This could enhance the functionality of your code and help you debug. Also notice that **a numpy array in shape $(N,1)$ is NOT the same as that in shape $(N,)$** so be careful and consistent on what you are using. You can see the detailed explanation here. [Difference between numpy.array shape (R, 1) and (R,)](https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r)\n",
    " - The dataset $X$: $(N, D)$\n",
    " - $\\mu$: $(K, D)$. \n",
    " - $\\Sigma$: $(K, D, D)$\n",
    " - $\\tau$: $(N, K)$\n",
    " - $\\pi$: array of length $K$\n",
    " - ll_joint: $(N, K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Helper functions [15 pts]\n",
    "\n",
    "To facilitate some of the operations in the GMM implementation, we would like you to implement the following three helper functions. In these functions, \"logit\" refers to an input array of size $(N, D)$. Remember the goal of helper functions is to facilitate our calculation so **DO NOT USE FOR LOOP ON N**.\n",
    "\n",
    "### 3.1.1. softmax [5 pts]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $prob \\in \\mathbb{R}^{N \\times D}$, where $prob_{i, j} = \\frac{\\exp(logit_{i, j})}{\\sum_{d=1}^D exp(logit_{i, d})}$.\n",
    "\n",
    "Note: it is possible that $logit_{i, j}$ is very large, making $\\exp(\\cdot)$ of it to explode. To make sure it is numerically stable, you need to subtract the maximum for each row of $logits$, and then add it back in your result.\n",
    "\n",
    "### 3.1.2. logsumexp [5 pts]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $s \\in \\mathbb{R}^N$, where $s_i = \\log \\big( \\sum_{j=1}^D \\exp(logit_{i, j}) \\big)$. Again, pay attention to the numerical problem. You may want to use similar trick as in the softmax function. Note: This function is used in the call() function which is given, so you will not need it in your own implementation. It helps calculate the loss of log-likehood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.3. Multivariate Gaussian PDF [5 pts]\n",
    "You should be able to write your own function based on the following formula, and you are NOT allowed to use outside resource packages other than those we provided. \n",
    "\n",
    "**(for undergrads only) normalPDF**\n",
    "\n",
    "Using the covariance matrix as a diagonal matrix with variances of the individual variables appearing on the main diagonal of the matrix and zeros everywhere else means that we assume the features are independent. In this case, the multivariate normal density function simplifies to the expression below:\n",
    "$$\\mathcal{N}(x: \\mu, \\Sigma) = \\prod_{i=1}^D \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{\\left( -\\frac{1}{2\\sigma_i^2} (x_i-\\mu_i)^2\\right)}$$\n",
    "where $\\sigma^2_i$ is the variance for the $i^{th}$ feature, which is the diagonal element of the covariance matrix.\n",
    "\n",
    "**(for grads only) multinormalPDF**\n",
    "\n",
    "Given the dataset $X \\in \\mathbb{R}^{N \\times D}$, the mean vector $\\mu \\in \\mathbb{R}^{D}$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{D \\times D}$ for a multivariate Gaussian distrubution, calculate the probability $p \\in \\mathbb{R}^{N}$ of each data. The PDF is given by \n",
    "$$\\mathcal{N}(X: \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}}|\\Sigma|^{-1/2}\\exp{\\left(-\\frac{1}{2}(X-\\mu)\\Sigma^{-1}(X-\\mu)^T\\right)}$$\n",
    "where $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "\n",
    "\n",
    "**Hints**\n",
    "- If you encounter \"LinAlgError\", you can mitigate your number/array by summing a small value before taking the operation, e.g. np.linalg.inv(\\$\\Sigma_k$ + 1e-32). You can arrest and handle such error by using [Try and Exception Block](https://realpython.com/python-exceptions/#the-try-and-except-block-handling-exceptions) in Python.\n",
    "\n",
    "- In the above calculation, you must avoid computing a $(N,N)$ matrix. Using the above equation for large N will crash your kernel and/or give you a memory error on Gradescope. Instead, you can do this same operation by calculating $(X-\\mu)\\Sigma^{-1}$, a $(N,D)$ matrix, transpose it to be a $(D,N)$ matrix and do an element-wise multiplication with $(X-\\mu)^T$, which is also a $(D,N)$ matrix. Lastly, you will need to sum over the 0 axis to get a $(1,N)$ matrix before proceeding with the rest of the calculation. This uses the fact that doing an element-wise multiplication and summing over the 0 axis is the same as taking the diagonal of the $(N,N)$ matrix from the matrix multiplication. \n",
    "- In Numpy implementation for $\\mu$, you can either use a 2-D array with dimension $(1,D)$ for each Gaussian Distribution, or a 1-D array with length $D$. Same to other array parameters. Both ways should be acceptable but pay attention to the shape mismatch problem and be **consistent all the time** when you implement such arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 GMM Implementation [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do in this problem:\n",
    "### 3.2.1. Initialize parameters in _init_components() [5 pts]\n",
    "\n",
    "Examples of how you can initialize the parameters. \n",
    "  1. Set the prior probability $\\pi$ the same for each class.\n",
    "  2. Initialize $\\mu$ by randomly selecting K numbers of observations as the initial mean vectors, and initialize the covariance matrix with np.eye() for each k. For grads, you can also initialize the $\\Sigma$ by K diagonal matrices. It will become a full matrix after one iteration, as long as you adopt the correct computation.\n",
    "  3. Other ways of initialization are acceptable and welcome.\n",
    "\n",
    "### 3.2.2. Formulate the log-likelihood function _ll_joint() [5 pts]\n",
    "\n",
    "The log-likelihood function is given by:\n",
    "\\begin{align}\n",
    "    \\ell(\\theta) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}\n",
    "In this part, we will generate a $(N,K)$ matrix where each datapoint $x_i, \\forall i = 1, \\dots, N$ has $K$ log-likelihood numbers. Thus, for each $i = 1, \\dots, N$ and $k = 1, \\dots, K$, \n",
    "$$\n",
    "\\text{log-likelihood}[i,k] = \\log{\\pi_k}+\\log{\\cal{N}(x_i|\\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "**Hints:**\n",
    "- If you encounter \"ZeroDivisionError\" or \"RuntimeWarning: divide by zero encountered in log\", you can mitigate your number/array by summing a small value before taking the operation, e.g. np.log(\\$\\pi_k$ + 1e-32). \n",
    "- You need to use the Multivariate Normal PDF function you created in the last part. Remember the PDF function is for each Gaussian Distribution (i.e. for each k) so you need to use a for loop over K. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Setup Iterative steps for EM Algorithm  [5+10 pts]\n",
    "\n",
    "You can find the detail instruction in the above description box. \n",
    "\n",
    "**Hints:**\n",
    "- For E steps, we already get the log-likelihood at _ll_joint() function. This is not the same as responsibilities ($\\tau$), but you should be able to finish this part with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "- For undergrads: Try to simplify your calculation for $\\Sigma$ in M steps as you assumed independent components. Make sure you are only taking the diagonal terms of your calculated covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, X, K, max_iters = 100): # No need to change\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            X: the observations/datapoints, N x D numpy array\n",
    "            K: number of clusters/components\n",
    "            max_iters: maximum number of iterations (used in EM implementation)\n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.max_iters = max_iters\n",
    "        \n",
    "        self.N = self.points.shape[0]        #number of observations\n",
    "        self.D = self.points.shape[1]        #number of features\n",
    "        self.K = K                           #number of components/clusters\n",
    "\n",
    "    #Helper function for you to implement\n",
    "    def softmax(self, logit): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit: N x D numpy array\n",
    "        Return:\n",
    "            prob: N x D numpy array. See the above function.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def logsumexp(self, logit): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit: N x D numpy array\n",
    "        Return:\n",
    "            s: N x 1 array where s[i,0] = logsumexp(logit[i,:]). See the above function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #for undergraduate student\n",
    "    def normalPDF(self, logit, mu_i, sigma_i): #[5pts]\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            logit: N x D numpy array\n",
    "            mu_i: 1xD numpy array (or array of lenth D), the center for the ith gaussian.\n",
    "            sigma_i: 1xDxD 3-D numpy array (or DxD 2-D numpy array), the covariance matrix of the ith gaussian.  \n",
    "        Return:\n",
    "            pdf: 1xN numpy array (or array of length N), the probability distribution of N data for the ith gaussian\n",
    "            \n",
    "        Hint: \n",
    "            np.diagonal() should be handy.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #for grad students\n",
    "    def multinormalPDF(self, logits, mu_i, sigma_i):  #[5pts]\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            logit: N x D numpy array\n",
    "            mu_i: 1xD numpy array (or array of lenth D), the center for the ith gaussian.\n",
    "            sigma_i: 1xDxD 3-D numpy array (or DxD 2-D numpy array), the covariance matrix of the ith gaussian.  \n",
    "        Return:\n",
    "            pdf: 1xN numpy array (or array of length N), the probability distribution of N data for the ith gaussian\n",
    "         \n",
    "        Hint: \n",
    "            np.linalg.det() and np.linalg.inv() should be handy.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def _init_components(self, **kwargs): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kwargs: any other arguments you want\n",
    "        Return:\n",
    "            pi: numpy array of length K, prior\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def _ll_joint(self, pi, mu, sigma, **kwargs): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "            \n",
    "        Return:\n",
    "            ll(log-likelihood): NxK array, where ll(i, k) = log pi(k) + log NormalPDF(points_i | mu[k], sigma[k])\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _E_step(self, pi, mu, sigma, **kwargs): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "        Return:\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            \n",
    "        Hint: \n",
    "            You should be able to do this with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _M_step(self, gamma, **kwargs): # [10pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "        Return:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "            \n",
    "        Hint:  \n",
    "            There are formulas in the slide and in the above description box.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def __call__(self, abs_tol=1e-16, rel_tol=1e-16, **kwargs): # No need to change\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            kwargs: any additional arguments you want\n",
    "        \n",
    "        Return:\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            (pi, mu, sigma): (1xK np array, KxD numpy array, KxDxD numpy array)       \n",
    "        \n",
    "        Hint: \n",
    "            You do not need to change it. For each iteration, we process E and M steps, then update the paramters. \n",
    "        \"\"\"\n",
    "        pi, mu, sigma = self._init_components(**kwargs)\n",
    "        pbar = tqdm(range(self.max_iters))\n",
    "        \n",
    "        for it in pbar:\n",
    "            # E-step\n",
    "            gamma = self._E_step(pi, mu, sigma)\n",
    "            \n",
    "            # M-step\n",
    "            pi, mu, sigma = self._M_step(gamma)\n",
    "            \n",
    "            # calculate the negative log-likelihood of observation\n",
    "            joint_ll = self._ll_joint(pi, mu, sigma)\n",
    "            loss = -np.sum(self.logsumexp(joint_ll))\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    break\n",
    "            prev_loss = loss\n",
    "            pbar.set_description('iter %d, loss: %.4f' % (it, loss))\n",
    "        return gamma, (pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Japanese art and pixel clustering [10pts + 5pts]\n",
    "\n",
    "Ukiyo-e is a Japanese art genre predominant from the 17th through 19th centuries. In order to produce the intricate prints that came to represent the genre, artists carved wood blocks with the patterns for each color in a design. Paint would be applied to the block and later transfered to the print to form the image.\n",
    "In this section, you will use your GMM algorithm implementation to do pixel clustering and estimate how many wood blocks were likely used to produce a single print. That is to say, how many wood blocks would appropriatly produce the original paint. \n",
    "(Hint: you can justify your answer based on visual inspection of the resulting images or on a different metric of your choosing)\n",
    "#### You do NOT need to submit your code for this question to the autograder. Instead you should include whatever images/information you find relevant in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for performing pixel clustering. You don't have to modify it\n",
    "def cluster_pixels_gmm(image, K):\n",
    "    \"\"\"Clusters pixels in the input image\n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "    Return:\n",
    "        clustered_img: image of shape(H, W, 3) after pixel clustering\n",
    "    \"\"\"\n",
    "    im_height, im_width, im_channel = image.shape\n",
    "    flat_img = np.reshape(image, [-1, im_channel]).astype(np.float32)\n",
    "    gamma, (pi, mu, sigma) = GMM(flat_img, K = K, max_iters = 100)()\n",
    "    cluster_ids = np.argmax(gamma, axis=1)\n",
    "    centers = mu\n",
    "\n",
    "    gmm_img = np.reshape(centers[cluster_ids], (im_height, im_width, im_channel))\n",
    "    \n",
    "    return gmm_img\n",
    "\n",
    "# helper function for plotting images. You don't have to modify it\n",
    "def plot_images(img_list, title_list, figsize=(20, 10)):\n",
    "    assert len(img_list) == len(title_list)\n",
    "    fig, axes = plt.subplots(1, len(title_list), figsize=figsize)\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(img_list[i] / 255.0)\n",
    "        ax.set_title(title_list[i])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 2 of the images in this list:\n",
    "url0 = 'https://upload.wikimedia.org/wikipedia/commons/b/b1/Utagawa_Kunisada_I_%28c._1832%29_Dawn_at_Futami-ga-ura.jpg'\n",
    "url1 = 'https://upload.wikimedia.org/wikipedia/commons/9/95/Hokusai_%281828%29_Cuckoo_and_Azaleas.jpg'\n",
    "url2 = 'https://upload.wikimedia.org/wikipedia/commons/7/74/Kitao_Shigemasa_%281777%29_Geisha_and_a_servant_carrying_her_shamisen_box.jpg'\n",
    "url3 = 'https://upload.wikimedia.org/wikipedia/commons/1/10/Kuniyoshi_Utagawa%2C_Suikoden_Series_4.jpg'\n",
    "\n",
    "# example of loading image from url0\n",
    "image = imageio.imread(imageio.core.urlopen(url0).read())\n",
    "\n",
    "# this is for you to implement\n",
    "def find_n_woodblocks(image, min_clusters=5, max_clusters=15):\n",
    "    \"\"\"\n",
    "    Using the helper function above to find the optimal number of woodblocks that can appropriatly produce a single image.\n",
    "    You can simply examinate the answer based on your visual inspection (i.e. looking at the resulting images) or provide any metrics you prefer. \n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        min_clusters, max_clusters: the minimum and maximum number of clusters you should test with. Default are 5a dn 15.\n",
    "        (Usually the maximum number of clusters would not exeed 15)\n",
    "        \n",
    "    Return:\n",
    "        plot: comparison between original image and image pixel clustering.\n",
    "        optional: any other information/metric/plot you think is necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus for All) [5 pts]\n",
    "Compare the full covariance matrix with the diagonal covariance matrix in GMM. Can you explain why the images are different with the same clusters?\n",
    "Note: You will have to implement both multinormalPDF and normalPDF, and add a few arguments in the original _ll_joint() and _Mstep() function to indicate which matrix you are using. You will earn full credit only if you implement both functions AND explain the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_matrix(image, K):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "        \n",
    "    Return:\n",
    "        plot: comparison between full covariance matrix and diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_matrix(image1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Bonus for Grad and Undergrad) A Wrench in the Machine [30pts]\n",
    "\n",
    "Learning to work with messy data is a hallmark of a well-rounded data scientist. In most real-world settings the data given will usually have some issue, so it is important to learn skills to work around such impasses. This part of the assignment looks to expose you to clever ways to fix data using concepts that you have already learned in the prior questions.\n",
    "\n",
    "The two solutions covered:\n",
    "    \n",
    "    KNN Algorithm Approach\n",
    "    EM Algorithm Approach\n",
    "\n",
    "\n",
    "#### Question\n",
    "You are a consultant assigned to a company which refines raw materials. To refine the raw materials necessary for their operations, the company owns a vast fleet of machines. Stressing the importance of having minimum down time for refining, you have been tasked to find a way to predict whether a machine will need to be repaired or not. In order to aid you on the task, the company has supplied you with historical telemetric data from all of the machines. The features range from averages of temperature, frequencies, and other salient observations of the units. The specifics of the features are not pertinent to the classification; it can be assured that each feature is statistically significant. A unit is given a 1 if it is broken and a 0 otherwise.\n",
    "\n",
    "However, due to a software bug in logging the telemetric data, 20% of the entries are missing labels and 30% are missing characterization data. Since simply removing the corrupted entries would not reflect the true variance of the data, your job is to implement a solution to clean the data so it can be properly classified. \n",
    "\n",
    "Your job is to assist the company in cleaning their data and implementing a semi-supervised learning framework to help them create a general classifier.\n",
    "\n",
    "You are given two files for this task:\n",
    "* telemetry_data.csv: the entire dataset with complete and incomplete data\n",
    "* validation_data.csv: a smaller, fully complete dataset made after the software bug had been fixed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.a Data Cleaning\n",
    "The first step is to break up the whole dataset into clear parts. All the data is randomly shuffled in one csv file. In order to move forward, the data needs to be split into three separate arrays: \n",
    "* labeled_complete: containing the complete characterization data and corresponding labels (broken = 1 and OK = 0)\n",
    "* labeled_incomplete: containing partial characterization data and corresponding labels (broken = 1 and OK = 0)\n",
    "* unlabeled_complete: containing only complete material characterization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: N x D numpy array    \n",
    "    Return:\n",
    "        labeled_complete: n x D array where values contain both complete features and labels\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def incomplete_(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: N x D numpy array    \n",
    "    Return:\n",
    "        labeled_incomplete: n x D array where values contain incomplete features but complete labels\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def unlabeled_(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: N x D numpy array    \n",
    "    Return:\n",
    "        unlabeled_complete: n x D array where values contain complete features but incomplete labels\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.b KNN [10pts]\n",
    "The second step in this task is to clean the Labeled_incomplete dataset by filling in the missing values with probable ones derived from complete data. A useful approach to this type of problem is using a k-nearest neighbors (k-NN) algorithm. For this application, the method consists of replacing the missing value of a given point with the mean of the closest k-neighbors to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def pairwise_dist(self, x, y): # [0pts] - copy from kmeans\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "            y: M x D numpy array\n",
    "        Return:\n",
    "            dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
    "            x[i, :] and y[j, :]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, incomplete_points,  complete_points, K, **kwargs): # [10pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            incomplete_points: N_incomplete x (D+1) numpy array, the incomplete labeled observations\n",
    "            complete_points: N_complete x (D+1) numpy array, the complete labeled observations\n",
    "            K: integer, corresponding to the number of nearest neighbors you want to base your calculation on\n",
    "            kwargs: any other args you want\n",
    "        Return:\n",
    "            clean_points: (N_incomplete + N_complete) x (D-1) X D numpy array of length K, containing both complete points and recently filled points\n",
    "            \n",
    "        Hints: (1) You want to find the k-nearest neighbors within each class separately;\n",
    "               (2) There are missing values in all of the features. It might be more convenient to address each feature at a time.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a good expectation of what the process should look like on a toy dataset. If your output matches the answer below, you are on the right track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = np.array([[1.,2.,3.,1],[7.,8.,9.,0],[16.,17.,18.,1],[22.,23.,24.,0]])\n",
    "incomplete_data = np.array([[1.,np.nan,3.,1],[7.,np.nan,9.,0],[np.nan,17.,18.,1],[np.nan,23.,24.,0]])\n",
    "\n",
    "clean_data = CleanData()(incomplete_data, complete_data, 2)\n",
    "print(\"*** Expected Answer - k = 2 ***\")\n",
    "print(\"\"\"==complete data==\n",
    "[[ 1.  5.  3.  1.]\n",
    " [ 7.  8.  9.  0.]\n",
    " [16. 17. 18.  1.]\n",
    " [22. 23. 24.  0.]]\n",
    "==incomplete data==\n",
    "[[ 1. nan  3.  1.]\n",
    " [ 7. nan  9.  0.]\n",
    " [nan 17. 18.  1.]\n",
    " [nan 23. 24.  0.]]\n",
    "==clean_data==\n",
    "[[ 1.   2.   3.   1. ]\n",
    " [ 7.   8.   9.   0. ]\n",
    " [16.  17.  18.   1. ]\n",
    " [22.  23.  24.   0. ]\n",
    " [14.5 23.  24.   0. ]\n",
    " [ 7.  15.5  9.   0. ]\n",
    " [ 8.5 17.  18.   1. ]\n",
    " [ 1.   9.5  3.   1. ]]\"\"\")\n",
    "\n",
    "print(\"\\n*** My Answer - k = 2***\")\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Getting acquainted with semi-supervised learning approaches. [5pts]\n",
    "\n",
    "You will implement a version of the algorithm presented in Table 1 of the paper [\"Text Classification from Labeled and Unlabeled Documents using EM\"](http://www.kamalnigam.com/papers/emcat-mlj99.pdf) by Nigam et al. (2000). While you are recommended to read the whole paper this assignment focuses on items 1$-$5.2 and 6.1. Write a brief summary of three interesting highlights of the paper (50-word maximum).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementing the EM algorithm. [10 pts]\n",
    "In your implementation of the EM algorithm proposed by Nigam et al. (2000) on Table 1, you will use a Gaussian Naive Bayes (GNB) classifier as opposed to a naive Bayes (NB) classifier. (Hint: Using a GNB in place of an NB will enable you to reuse most of the implementation you developed for GMM in this assignment. In fact, you can successfully solve the problem by simply modifying the call method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiSupervised(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def softmax(self,logits): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        logits: N x D numpy array\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def logsumexp(self,logits): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: N x D numpy array\n",
    "        Return:\n",
    "            s: N x 1 array where s[i,0] = logsumexp(logits[i,:])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _init_components(self, points, K, **kwargs): # [5 pts] - modify from GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: Nx(D+1) numpy array, the observations\n",
    "            K: number of components\n",
    "            kwargs: any other args you want\n",
    "        Return:\n",
    "            pi: numpy array of length K, prior\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "            \n",
    "        Hint: The paper describes how you should initialize your algorithm.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _ll_joint(self, points, pi, mu, sigma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "        Return:\n",
    "            ll(log-likelihood): NxK array, where ll(i, j) = log pi(j) + log NormalPDF(points_i | mu[j], sigma[j])\n",
    "            \n",
    "        Hint: Assume that the three properties of the lithium-ion batteries (multivariate gaussian) are independent.  \n",
    "              This allows you to treat it as a product of univariate gaussians.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _E_step(self, points, pi, mu, sigma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "        Return:\n",
    "            gamma: NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            \n",
    "        Hint: You should be able to do this with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _M_step(self, points, gamma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            gamma: NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "        Return:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. \n",
    "            \n",
    "        Hint:  There are formulas in the slide.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, **kwargs): # [5 pts] - modify from GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            max_iters: maximum number of iterations\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            gamma: NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            (pi, mu, sigma): (1xK np array, KxD numpy array, KxD numpy array), mu and sigma.\n",
    "         \n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Demonstrating the performance of the algorithm. [5pts]\n",
    "Compare the classification error based on the Gaussian Naive Bayes (GNB) classifier you implemented following the Nigam et al. (2000) approach to the performance of a GNB classifier trained using only labeled data. Since you have not covered supervised learning in class, you are allowed to use the scikit learn library for training the GNB classifier based only on labeled data: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ComparePerformance(object):\n",
    "    \n",
    "    def __init__(self): #No need to implement\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def accuracy_semi_supervised(self, points, independent, n=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: Nx(D+1) numpy array, where N is the number of points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels (when available) or a flag that allows you to separate the unlabeled data.\n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def accuracy_GNB_onlycomplete(self, points, independent, n=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: Nx(D+1) numpy array, where N is the number of only initially complete labeled points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels.\n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def accuracy_GNB_cleandata(self, points, independent, n=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: Nx(D+1) numpy array, where N is the number of clean labeled points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels.\n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load and clean data for the next section\n",
    "telemetry = np.loadtxt('data/telemetry.csv', delimiter=',')\n",
    "\n",
    "labeled_complete = complete_(telemetry)\n",
    "labeled_incomplete = incomplete_(telemetry)\n",
    "unlabeled = unlabeled_(telemetry)\n",
    "\n",
    "clean_data = CleanData()(labeled_incomplete, labeled_complete, 7)\n",
    "# load unlabeled set\n",
    "# append unlabeled flag\n",
    "unlabeled_flag = -1*np.ones((unlabeled.shape[0],1))\n",
    "unlabeled = np.concatenate((unlabeled, unlabeled_flag), 1)\n",
    "unlabeled = np.delete(unlabeled, -1, axis=1)\n",
    "\n",
    "# =========================================================================\n",
    "# SEMI SUPERVISED\n",
    "\n",
    "# format training data\n",
    "points = np.concatenate((clean_data, unlabeled),0)\n",
    "\n",
    "# train model\n",
    "(pi, mu, sigma) = SemiSupervised()(points, 7)\n",
    "\n",
    "# ==========================================================================\n",
    "# COMPARISON\n",
    "\n",
    "# load test data\n",
    "independent = np.loadtxt('data/validation.csv', delimiter=',')\n",
    "\n",
    "# classify test data\n",
    "classification = SemiSupervised()._E_step(independent[:,:8], pi, mu, sigma)\n",
    "classification = np.argmax(classification,axis=1)\n",
    "\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\"\"===COMPARISON===\"\"\")\n",
    "print(\"\"\"SemiSupervised Accuracy:\"\"\", ComparePerformance().accuracy_semi_supervised(classification, independent))\n",
    "print(\"\"\"Supervised with clean data: GNB Accuracy:\"\"\", ComparePerformance().accuracy_GNB_onlycomplete(labeled_complete, independent))\n",
    "print(\"\"\"Supervised with only complete data: GNB Accuracy:\"\"\", ComparePerformance().accuracy_GNB_cleandata(clean_data, independent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}