{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:17:25.168179Z",
     "start_time": "2019-06-20T17:17:24.986130Z"
    },
    "id": "Hjp__3bRh42K",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Fall 2020 CS 4641\\7641 A: Machine Learning Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: November 18th, Wednesday, AOE\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJJRXYL2yOY"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- In this assignment, we have programming and writing questions.\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "- Typing with Latex\\markdown is required for all the written questions. Handwritten answers would not be accepted. \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "- Questions marked with <span style=\"color:blue\">**[P]**</span> are programming only and should be submitted to the autograder. Questions marked with <span style=\"color:green\">**[W]**</span> may required that you code a small function or generate plots, but should **NOT** be submitted to the autograder. It should be submitted on the writing portion of the assignment on gradescope\n",
    "- The outline of the assignment is as follows:\n",
    "    * Q1 [55+(10 bonus for undergrads)] > Neural Network <span style=\"color:green\">**[W]** </span>| <span style=\"color:blue\">**[P]**</span>\n",
    "    * Q2 [15 pts(bonus for all)] > Image Classification based on Convolutional Neural Network </span>| <span style=\"color:green\">**[W]**</span>\n",
    "    * Q3 [40 pts] > Random Forest <span style=\"color:blue\">**[P]** 3.1, 3.2 </span> | <span style=\"color:green\">**[W]**</span> 3.3\n",
    "    * Q4 [30 pts] > SVM <span style=\"color:green\">**[W]** </span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN-9x-Qm2yOY"
   },
   "source": [
    "## Using the autograder\n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW4: \"HW4 - Programming\" and \"HW4 - Non-programming\" (and \"HW4 - Bonus Programming\" if you are in CS4641).\n",
    "- You will submit your code for the autograder on \"HW4 - Programming\" in the following format:\n",
    "    \n",
    "    * random_forest.py\n",
    "    * neural_network.py\n",
    "- You will submit your code for the autograder on \"HW4 - Bonus-Programming\" in the following format:\n",
    "    \n",
    "    * neural_network.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes \"dlnet\" and \"RandomForest\" onto the respective files. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "\n",
    "- **For the \"HW4 - Non-programming\" part, you will download your jupyter notbook as HTML, print it as a PDF from your browser and submit it on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > HTML\". The non-programming part corresponds to Q1, Q2, Q3.3, Q4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o0Ui6T2as9iI"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq4QZ4su2yOd"
   },
   "source": [
    "## 1. Two Layer Neural Network [65pts] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "### Perceptron\n",
    "\n",
    "![Perceptron](https://drive.google.com/uc?id=1k_ITywpGxTqXd12fYZGstjj6A5H_Lk-w)\n",
    "<br><br>\n",
    "\n",
    "A single layer perceptron can be thought of as a linear hyperplane as in logistic regression followed by a non-linear activation function. $$u_{i} = \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i}$$  $$o_{i} = \\phi \\left( \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i} \\right) = \\phi(\\theta_{i}^{T}x+b_{i})$$ where $x$ is a d-dimensional vector i.e. $x \\in R^{d}$. It is one datapoint with $d$ features. $\\theta_{i} \\in R^{d}$ is the weight vector for the $i^{th}$ hidden unit, $b_{i} \\in R$ is the bias element for the $i^{th}$ hidden unit and $\\phi(.)$ is a non-linear activation function that has been described below. $u_{i}$ is a linear combination of the features in $x$ weighted by $\\theta_{i}$ whereas $o_{i}$ is the $i^{th}$ output unit from the activation layer. \n",
    "\n",
    "\n",
    "## Fully connected Layer\n",
    "Typically, a modern neural network contains millions of perceptrons as the one shown in the previous image. Perceptrons interact in different configurations such as cascaded or parallel. In this part, we describe a fully connected layer configuration in a neural network which comprises multiple parallel perceptrons forming one layer. \n",
    "\n",
    "We extend the previous notation to describe a fully connected layer. Each layer in a fully connected network has a number of input/hidden/output units cascaded in parallel. Let us a define a single layer of the neural net as follows: <br>\n",
    "$m$ demotes the number of hidden units in a single layer $l$ whereas $n$ denotes the number of units in the previous layer $l-1$.\n",
    "$$u^{[l]}=\\theta^{[l]}o^{[l-1]}+b^{[l]}$$ where $u^{[l]} \\in R^{m}$ is a m-dimensional vector pertaining to the hidden units of the $l^{th}$ layer of the neural network after applying linear operations. Similarly, $o^{[l-1]}$ is the n-dimensional output vector corresponding to the hidden units of the $(l-1)^{th}$ activation layer. $\\theta^{[l]} \\in R^{m \\times n}$ is the weight matrix of the $l^{th}$ layer where each row of $\\theta^{[l]}$ is analogous to $\\theta_{i}$ described in the previous section i.e. each row corresponds to one hidden unit of the $l^{th}$ layer. $b^{[l]} \\in R^{m}$ is the bias vector of the layer where each element of b pertains to one hidden unit of the $l^{th}$ layer. This is followed by element wise non-linear activation function $o^{[l]} = \\phi(u^{[l]})$.\n",
    "The whole operation can be summarized as,\n",
    "$$o^{[l]} = \\phi(\\theta^{[l]}o^{[l-1]}+b^{[l]}) $$\n",
    "where $o^{[l-1]}$ is the output of the previous layer. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Activation Function \n",
    "There are many activation functions in the literature but for this question we are going to use Relu, Sigmoid and Tanh only. \n",
    "### Relu\n",
    "The rectified linear unit (Relu) is one of the most commonly used activation functions in deep learning models. The mathematical form is $$o = \\phi(u) = max(0,u)$$<br> The derivative of relu function is given as $o' = \\phi'(u) = \\begin{cases}\n",
    "&0& u \\leq 0 \\\\\n",
    "&1& u > 0\n",
    "\\end{cases} $  \n",
    "\n",
    "\n",
    "![Relu](https://drive.google.com/uc?id=1qFNOnhB3B0wgt56bMsc-WI6cX0UWJAQX)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Sigmoid\n",
    "The sigmoid function is another non-linear function with S-shaped curve. This function is useful in the case of binary classification as its output is between 0 and 1. The mathematical form of the function is $$o = \\phi(u)=\\frac{1}{1+e^{-u}}$$<br> The derivation of the sigmoid function has a nice form and is given as $$o' = \\phi'(u) = \\frac{1}{1+e^{-u}} \\left(1-\\frac{1}{1+e^{-u}}\\right) = \\phi(u)(1-\\phi(u))$$\n",
    "<br><br>\n",
    "\n",
    "![Sigmoid](https://drive.google.com/uc?id=19UPS1IfcVNqH_PMAPg6ymAAVqGo9zHle)\n",
    "\n",
    "### Tanh\n",
    "Tanh also known as hyperbolic tangent is like a shifted version of sigmoid activation function with its range going from -1 to 1. Tanh almost always proves to be better than the sigmoid function since the mean of the activations are closer to zero. Tanh has an effect of centering data that makes learning for the next layer a bit easier. The mathematical form of tanh is given as $$o = \\phi(u) = tanh(u) = \\frac{e^{u} - e^{-u}}{e^{u} + e^{-u}}$$ The derivative of tanh is given as $$o' = \\phi'(u) = 1 - {\\left(\\frac{e^{u} - e^{-u}}{e^{u} + e^{-u}}\\right)}^{2} = 1 - o^{2}$$\n",
    "\n",
    "![Tanh](https://drive.google.com/uc?id=1FD83cZIsI1gY6g0dDKyaaHFrCnThl9za)\n",
    "\n",
    "\n",
    "## Cross Entropy Loss\n",
    "An essential piece in training a neural network is the loss function. The whole purpose of gradient descent algorithm is to find some network parameters that minimizes the loss function. In this exercise, we minimize Cross Entropy (CE) loss that represents on an intuitive level the distance between true data distribution and estimated distribution by neural network. So during training of the neural network, we will be looking for network parameters that minimizes the distance between true and estimated distribution. The mathematical form of the CE loss is given by \n",
    "$$CE(p,q) = -\\sum\\limits_{i} p(x_{i})\\log q(x_{i}) $$\n",
    "where $p(x)$ is the true distribution and $q(x)$ is the estimated distribution. \n",
    "### Implementation details\n",
    "For binary classification problems as in this exercise, we have probability distribution of a label $y_{i}$ given by\n",
    "\\begin{equation}\n",
    "y_{i}= \n",
    "\\begin{cases}\n",
    "&1& \\text{ with probability } p(x_{i}) \\\\\n",
    "&0& \\text{ with probability } 1- p(x_{i})\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "A frequentist estimate of $p(x_{i})$ can be written as $$p(x_{i})= \\sum\\limits_{i=1}^{N} \\frac{y_{i}}{N}$$ Therefore, the cross entropy for binary estimation can be written as \n",
    "$$CE(y_{i},\\hat{y_{i}}) = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(y_{i} \\log (\\hat{y_{i}}) +(1-y_{i}) \\log (1-\\hat{y_{i}})\\right)$$\n",
    "where $y_{i} \\in \\{ 0,1\\}$ is the true label and $\\hat{y_{i}} \\in [0,1]$ is the estimated label.  \n",
    "\n",
    "## Forward Propagation\n",
    "We start by initializing the weights of the fully connected layer using Xavier initialization [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). During training, we pass all the data points through the network layer by layer using forward propagation. The main equations for forward prop have been described below. \n",
    "\\begin{eqnarray}\n",
    "u^{[0]} &=& x\\\\\n",
    "u^{[1]}&=& \\theta^{[1]}u^{[0]}+b^{[1]} \\\\\n",
    "o^{[1]}&=& Relu(u^{[1]}) \\\\\n",
    "u^{[2]}&=& \\theta^{[2]}o^{[1]}+b^{[2]} \\\\\n",
    "\\hat{y}=o^{[2]}&=& Sigmoid(u^{[2]}) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Then we get the output and compute the loss \n",
    "$$l = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(y_{i} \\log (\\hat{y_{i}}) +(1-y_{i}) \\log (1-\\hat{y_{i}})\\right)$$\n",
    "\n",
    "\n",
    "## Backward propagation\n",
    "After the forward pass, we do back propagation to update the weights and biases in the direction of the negative gradient of the loss function. So, we update the weights and biases using the following formulas\n",
    "\\begin{equation}\n",
    "\\theta^{[2]} := \\theta^{[2]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - lr \\times \\frac{\\partial l}{\\partial b^{[2]}} \\\\\n",
    "\\theta^{[1]} := \\theta^{[1]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - lr \\times \\frac{\\partial l}{\\partial b^{[1]}}\n",
    "\\end{equation}\n",
    "where $lr$ is the learning rate. It decides the step size we want to take in the direction of the negative gradient. \n",
    "\n",
    "\n",
    "\n",
    "To compute the terms $\\frac{\\partial l}{\\partial \\theta^{[i]}}$ and $ \\frac{\\partial l}{\\partial b^{[i]}}$ we use chain rule for differentiation as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial \\theta^{[2]}}&=&\\frac{\\partial l}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial \\theta^{[2]}} \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[2]}}&=&\\frac{\\partial l}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial b^{[2]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "So, $\\frac{\\partial l}{\\partial o^{[2]}}$ is the differentiation of the cross entropy loss function at point $o^{[2]}$ <br><br> $\\frac{\\partial o^{[2]}}{\\partial u^{[2]}}$ is the differentiation of the Sigmoid function at point $u^{[2]}$ <br><br> $\\frac{\\partial u^{[2]}}{\\partial \\theta^{[2]}}$ is equal to $o^{[1]}$ <br><br> $\\frac{\\partial u^{[2]}}{\\partial b^{[2]}}$ is equal to $1$. <br><br>\n",
    "\n",
    "To compute $\\frac{\\partial l}{\\partial \\theta^{[2]}}$, we need $o^{[2]}, u^{[2]} \\& o^{[1]}$ which are calculated during forward propagation. So we need to store these values in cache variables during forward propagation to be able to access them during backward propagation. Similarly for calculating other partial derivatives, we store the values we'll be needing for chain rule in cache. These values are obtained from the forward propagation and used in backward propagation. The cache is implemented as a dictionary here where the keys are the variable names and the values are the variables values.  <br><br>Also, the functional form of the CE differentiation and Sigmoid differentiation are given by <br><br>\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial o^{[2]}} &=& \\frac{-1}{N}\\left(\\frac{y}{o^{[2]}}-\\frac{1-y}{1-o^{[2]}}\\right) \\\\\n",
    "\\frac{\\partial o^{[2]}}{\\partial u^{[2]}} &=& \\frac{1}{1+e^{-u^{[2]}}} \\left(1- \\frac{1}{1+e^{-u^{[2]}}} \\right) &=& o^{[2]}(1-o^{[2]})  \\\\\n",
    "\\frac{\\partial u^{[2]}}{\\partial \\theta^{[2]}} &=& o^{[1]} \\\\\n",
    "\\frac{\\partial u^{[2]}}{\\partial b^{[2]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "This completes the differentiation of loss function w.r.t to parameters in the second layer. We now move on to the first layer, the equations for which are given as follows: <br><br> \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial \\theta^{[1]}}&=&\\frac{\\partial l}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[1]}} \\frac{\\partial o^{[1]}}{\\partial u^{[1]}} \\frac{\\partial u^{[1]}}{\\partial \\theta^{[1]}}  \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[1]}}&=&\\frac{\\partial l}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[1]}} \\frac{\\partial o^{[1]}}{\\partial u^{[1]}} \\frac{\\partial u^{[1]}}{\\partial b^{[1]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial u^{[2]}}{\\partial o^{[1]}} &=& \\theta^{[2]} \\\\\n",
    "\\frac{\\partial o^{[1]}}{\\partial u^{[1]}} &=&  \n",
    "\\begin{cases}\n",
    "&0& \\text{ if } u^{[1]} \\leq 0 \\\\\n",
    "&1& \\text{ if } u^{[1]} > 0 \n",
    "\\end{cases}\\\\\n",
    "\\frac{\\partial u^{[1]}}{\\partial \\theta^{[1]}} &=& x\\\\\n",
    "\\frac{\\partial u^{[1]}}{\\partial b^{[1]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that $\\frac{\\partial o^{[1]}}{\\partial u^{[1]}}$ is the differentiation of the Relu function at $u^{[1]}$.\n",
    "\n",
    "The above equations outline the forward and backward propagation process for a 2-layer fully connected neural net with relu as the first activation layer and sigmoid has the second one. The same process can be extended to different neural networks with different activation layers like tanh. \n",
    "\n",
    "\n",
    "\n",
    "## Code Implementation: \n",
    "\n",
    "$$ \\begin{eqnarray} dLoss\\_o2 &=& \\frac{\\partial l}{\\partial o^{[2]}} \\implies dim=(1,426) \\\\ dLoss\\_u2 &=& dLoss\\_o2 \\frac{\\partial o^{[2]}}{\\partial u^{[2]}} \\implies dim=(1,426) \\\\ dLoss\\_theta2 &=& dLoss\\_u2 \\frac{\\partial u^{[2]}}{\\partial \\theta^{[2]}} \\implies dim=(1,15) \\\\ dLoss\\_b2 &=& dLoss\\_u2 \\frac{\\partial u^{[2]}}{\\partial b^{[2]}} \\implies dim=(1,1) \\\\ dLoss\\_o1 &=& dLoss\\_u2 \\frac{\\partial u^{[2]}}{\\partial o^{[1]}} \\implies dim=(15,426) \\\\ dLoss\\_u1 &=& dLoss\\_o1 \\frac{\\partial o^{[1]}}{\\partial u^{[1]}} \\implies dim=(15,426) \\\\ dLoss\\_theta1 &=& dLoss\\_u1 \\frac{\\partial u^{[1]}}{\\partial \\theta^{[1]}} \\implies dim=(15,30) \\\\ dLoss\\_b1 &=& dLoss\\_u1 \\frac{\\partial u^{[1]}}{\\partial b^{[1]}} \\implies dim=(15,1) \\end{eqnarray} $$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Question \n",
    "\n",
    "In this question, you will implement a two layer fully connected neural network. You will also experiment with different activation functions and optimization techniques. Functions with comments \"TODO: implement this\" are for you to implement. We provide three activation functions here - Relu, Tanh and Sigmoid. You will implement a neural network that could have relu activation followed by sigmoid layer or tanh activation followed by sigmoid. You'll have to specify the neural net type which could be \"Relu -> Sigmoid\" (set by default) or \"Tanh -> Sigmoid\". \n",
    "\n",
    "\n",
    "You'll also implement gradient descent and stochastic gradient descent algorithms for training these neural nets. SGD is bonus for undergraduate students. \n",
    "\n",
    "We'll train these neural nets on breast cancer dataset. You're free to choose either gradient descent or SGD for training. Note: it is possible you'll run into nan or negative values for loss. This happens because of the small dataset we're using and some numerical stability issues that arise due to division by zero, natural log of zeros etc. You can experiment with the total number of iterations to mitigate this. \n",
    "\n",
    "<b>Deliverables for this question: </b>\n",
    "1. Loss plot and classification report for any neural net type (\"Relu -> Sigmoid\" or \"Tanh -> Sigmoid\") with gradient descent \n",
    "2. Loss plot and classification report for any neural net type (\"Relu -> Sigmoid\" or \"Tanh -> Sigmoid\") with stochastic gradient descent (mandatory for graduate students, bonus for undergraduate students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eVcXl9-D2DMW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "to train a 2 fully connected layer neural net. We are going to buld the neural network from scratch. \n",
    "'''\n",
    "\n",
    "\n",
    "class dlnet:\n",
    "\n",
    "    def __init__(self, x, y, lr = 0.003):\n",
    "        '''\n",
    "        This method inializes the class, its implmented for you. \n",
    "        Args:\n",
    "            x: data\n",
    "            y: labels\n",
    "            Yh: predicted labels\n",
    "            dims: dimensions of different layers\n",
    "            param: dictionary of different layers parameters\n",
    "            ch: Cache dictionary to store forward parameters that are used in backpropagation\n",
    "            loss: list to store loss values\n",
    "            lr: learning rate\n",
    "            sam: number of training samples we have\n",
    "\n",
    "        '''        \n",
    "        self.X=x # features\n",
    "        self.Y=y # ground truth labels\n",
    "\n",
    "        self.Yh=np.zeros((1,self.Y.shape[1])) # estimated labels\n",
    "        self.dims = [30, 15, 1] # dimensions of different layers\n",
    "\n",
    "        self.param = { } # dictionary for different layer variables\n",
    "        self.ch = {} # cache for holding variables during forward propagation to use them in back prop\n",
    "        self.loss = []\n",
    "\n",
    "        self.lr=lr # learning rate\n",
    "        self.sam = self.Y.shape[1] # number of training samples we have\n",
    "        self._estimator_type = 'classifier'\n",
    "        self.neural_net_type = \"Tanh -> Sigmoid\" #can change it to \"Tanh -> Sigmoid\" \n",
    "\n",
    "    def nInit(self): \n",
    "        '''\n",
    "        This method initializes the neural network variables, its already implemented for you. \n",
    "        Check it and relate to mathematical the description above.\n",
    "        You are going to use these variables in forward and backward propagation.\n",
    "        '''   \n",
    "        np.random.seed(1)\n",
    "        self.param['theta1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
    "        self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
    "        self.param['theta2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
    "        self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
    "        \n",
    "    \n",
    "\n",
    "    def Relu(self, u):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Relu. \n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension. \n",
    "        Input: u of any dimension\n",
    "        return: Relu(u) \n",
    "        '''\n",
    "        relu = np.vectorize(lambda x: max(0, x))\n",
    "        o = relu(u)\n",
    "        return o\n",
    "\n",
    "    def Sigmoid(self, u): \n",
    "        '''\n",
    "        In this method you are going to implement element wise Sigmoid. \n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension. \n",
    "        Input: u of any dimension\n",
    "        return: Sigmoid(u) \n",
    "        '''\n",
    "        sigmoid = np.vectorize(lambda x: 1 / (1 + np.exp(-x)))\n",
    "        o = sigmoid(u)\n",
    "        return o\n",
    "\n",
    "\n",
    "    def Tanh(self, u):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Tanh. \n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension. \n",
    "        Input: u of any dimension\n",
    "        return: Tanh(u) \n",
    "        '''\n",
    "        tanh = np.vectorize(lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))\n",
    "        o = tanh(u)\n",
    "        return o\n",
    "    \n",
    "    \n",
    "    def dRelu(self, u):\n",
    "        '''\n",
    "        This method implements element wise differentiation of Relu.  \n",
    "        Input: u of any dimension\n",
    "        return: dRelu(u) \n",
    "        '''\n",
    "        u[u<=0] = 0\n",
    "        u[u>0] = 1\n",
    "        return u\n",
    "\n",
    "\n",
    "    def dSigmoid(self, u):\n",
    "        '''\n",
    "        This method implements element wise differentiation of Sigmoid.  \n",
    "        Input: u of any dimension\n",
    "        return: dSigmoid(u) \n",
    "        '''\n",
    "        o = 1/(1+np.exp(-u))\n",
    "        do = o * (1-o)\n",
    "        return do\n",
    "\n",
    "    def dTanh(self, u):\n",
    "        '''\n",
    "        This method implements element wise differentiation of Tanh. \n",
    "        Input: u of any dimension\n",
    "        return: dTanh(u) \n",
    "        '''\n",
    "        o = np.tanh(u)\n",
    "        return 1-o**2\n",
    "    \n",
    "    \n",
    "    def nloss(self,y, yh):\n",
    "        '''\n",
    "        In this method you are going to implement Cross Entropy loss. \n",
    "        Refer to the description above and implement the appropriate mathematical equation.\n",
    "        Input: y 1xN: ground truth labels\n",
    "               yh 1xN: neural network output after Sigmoid\n",
    "\n",
    "        return: CE 1x1: loss value \n",
    "        '''\n",
    "        CE = -1 * np.mean((np.multiply(y, np.log(yh)) + np.multiply((1 - y), np.log(1 - yh))))\n",
    "        return CE\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details.\n",
    "        Check nInit method and use variables from there as well as other implemented methods.\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        do not change the lines followed by #keep. \n",
    "        '''\n",
    "        self.ch['X'] = x #keep\n",
    "        u1 = np.dot(self.param['theta1'], self.ch['X']) + self.param['b1']\n",
    "        o1 = self.Tanh(u1)\n",
    "        self.ch['u1'], self.ch['o1'] = u1, o1 #keep\n",
    "        u2 = np.dot(self.param['theta2'], self.ch['o1']) + self.param['b2']\n",
    "        o2 = self.Sigmoid(u2)\n",
    "        self.ch['u2'], self.ch['o2'] = u2, o2 #keep\n",
    "        return o2 #keep\n",
    "\n",
    "\n",
    "    def backward(self, y, yh):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details\n",
    "        You will need to use cache variables, some of the implemented methods, and other variables as well\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        do not change the lines followed by #keep.  \n",
    "        '''\n",
    "        dLoss_o2 = - (np.divide(y, yh) - np.divide(1 - y, 1 - yh)) / y.shape[1]  # partial l by partial o2\n",
    "\n",
    "        # Implement equations for getting derivative of loss w.r.t u2, theta2 and b2\n",
    "        dLoss_u2 = np.multiply(dLoss_o2, np.multiply(self.ch['o2'], (1 - self.ch['o2'])))  # partial l by partial u2\n",
    "        dLoss_theta2 = np.matmul(dLoss_u2, self.ch['o1'].T)  # partial l by partial theta2\n",
    "        dLoss_b2 = np.matmul(dLoss_u2, np.ones((dLoss_u2.shape[1], 1)))  # partial l by partial b2\n",
    "\n",
    "        # set dLoss_u2, dLoss_theta2, dLoss_b2\n",
    "        self.ch['dLoss_u2'] = dLoss_u2\n",
    "        self.ch['dLoss_theta2'] = dLoss_theta2\n",
    "        self.ch['dLoss_b2'] = dLoss_b2\n",
    "\n",
    "        dLoss_o1 = np.matmul(self.param[\"theta2\"].T, dLoss_u2)  # partial l by partial o1\n",
    "\n",
    "        # Implement equations for getting derivative of loss w.r.t u1, theta1 and b1\n",
    "        do1_u1 = self.dTanh(self.ch['u1'])\n",
    "        dLoss_u1 = np.multiply(dLoss_o1, do1_u1)  # partial l by partial u1\n",
    "        dLoss_theta1 = np.matmul(dLoss_u1, self.ch['X'].T)  # partial l by partial theta1\n",
    "        dLoss_b1 = np.matmul(dLoss_u1, np.ones((dLoss_u1.shape[1], 1)))  # partial l by partial b1\n",
    "\n",
    "        # set dLoss_u1, dLoss_theta1, dLoss_b1\n",
    "        self.ch['dLoss_u1'] = dLoss_u1\n",
    "        self.ch['dLoss_theta1'] = dLoss_theta1\n",
    "        self.ch['dLoss_b1'] = dLoss_b1\n",
    "\n",
    "        # parameters update, no need to change these lines\n",
    "        self.param[\"theta2\"] = self.param[\"theta2\"] - self.lr * dLoss_theta2  # keep\n",
    "        self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2  # keep\n",
    "        self.param[\"theta1\"] = self.param[\"theta1\"] - self.lr * dLoss_theta1  # keep\n",
    "        self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1  # keep\n",
    "        return dLoss_theta2, dLoss_b2, dLoss_theta1, dLoss_b1\n",
    "\n",
    "\n",
    "    def gradient_descent(self, x, y, iter = 60000):\n",
    "        '''\n",
    "        This function is an implementation of the gradient decent algorithm \n",
    "        '''\n",
    "        self.nInit()\n",
    "        for i in range(iter):\n",
    "            yh = self.forward(x)\n",
    "            if i % 2000 == 0:\n",
    "                loss = self.nloss(y, yh)\n",
    "                self.loss.append(loss)\n",
    "                print('Loss after iteration %d: %6f' % (i, loss))\n",
    "            _, _, _, _ = self.backward(y, yh)\n",
    "\n",
    "\n",
    "    #bonus for undergraduate students \n",
    "    def stochastic_gradient_descent(self, x, y, iter = 60000):\n",
    "        '''\n",
    "        This function is an implementation of the stochastic gradient decent algorithm\n",
    "\n",
    "        Note: \n",
    "        1. SGD loops over all examples in the dataset one by one and learns a gradient from each example \n",
    "        2. One iteration here is one round of forward and backward propagation on one example of the dataset. \n",
    "           So if the dataset has 1000 examples, 1000 iterations will constitute an epoch \n",
    "        3. Append loss after every 2000 iterations for plotting loss plots\n",
    "        4. It is fine if you get a noisy plot since learning on one example at a time adds variance to the \n",
    "           gradients learnt\n",
    "        5. You can use SGD with any neural net type \n",
    "        '''\n",
    "        self.nInit()\n",
    "        for i in range(iter):\n",
    "            idx = i % x.shape[1]\n",
    "            xs = np.expand_dims(x[:, idx], axis=1)\n",
    "            ys = np.expand_dims(y[:, idx], axis=1)\n",
    "            yh = self.forward(xs)\n",
    "            if i % 2000 == 0:\n",
    "                loss = self.nloss(ys, yh)\n",
    "                self.loss.append(loss)\n",
    "                print('Loss after iteration %d: %6f' % (i, loss))\n",
    "            _, _, _, _ = self.backward(ys, yh)\n",
    "\n",
    "\n",
    "    def predict(self, x): \n",
    "        '''\n",
    "        This function predicts new data points\n",
    "        Its implemented for you\n",
    "        '''\n",
    "        Yh = self.forward(x)\n",
    "        return np.round(Yh).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "z-nbf6IJ2fkS",
    "outputId": "abedc38a-698f-4f2e-f2e2-05f4a7be29ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.693168\n",
      "Loss after iteration 2000: 0.084754\n",
      "Loss after iteration 4000: 0.064741\n",
      "Loss after iteration 6000: 0.061918\n",
      "Loss after iteration 8000: 0.052054\n",
      "Loss after iteration 10000: 0.055903\n",
      "Loss after iteration 12000: 0.046512\n",
      "Loss after iteration 14000: 0.037064\n",
      "Loss after iteration 16000: 0.037236\n",
      "Loss after iteration 18000: 0.038011\n",
      "Loss after iteration 20000: 0.042116\n",
      "Loss after iteration 22000: 0.043535\n",
      "Loss after iteration 24000: 0.042039\n",
      "Loss after iteration 26000: 0.038610\n",
      "Loss after iteration 28000: 0.031002\n",
      "Loss after iteration 30000: 0.031271\n",
      "Loss after iteration 32000: 0.033081\n",
      "Loss after iteration 34000: 0.034609\n",
      "Loss after iteration 36000: 0.023405\n",
      "Loss after iteration 38000: 0.023475\n",
      "Loss after iteration 40000: 0.029696\n",
      "Loss after iteration 42000: 0.029655\n",
      "Loss after iteration 44000: 0.036368\n",
      "Loss after iteration 46000: 0.021180\n",
      "Loss after iteration 48000: 0.026940\n",
      "Loss after iteration 50000: 0.022231\n",
      "Loss after iteration 52000: 0.021531\n",
      "Loss after iteration 54000: 0.030991\n",
      "Loss after iteration 56000: 0.023215\n",
      "Loss after iteration 58000: 0.021008\n",
      "Loss after iteration 60000: 0.014481\n",
      "Loss after iteration 62000: 0.023599\n",
      "Loss after iteration 64000: 0.029318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Training the Neural Network, you do not need to modify this cell\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "'''\n",
    "dataset = load_breast_cancer() # load the dataset\n",
    "x, y = dataset.data, dataset.target\n",
    "x = MinMaxScaler().fit_transform(x) #normalize data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1) #split data\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.reshape(1,-1), y_test #condition data\n",
    "\n",
    "nn = dlnet(x_train,y_train,lr=0.1) # initalize neural net class\n",
    "nn.gradient_descent(x_train, y_train, iter = 66000) #train\n",
    "\n",
    "# create figure\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f'Training: {nn.neural_net_type}')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "DlHFnPF92yOp",
    "outputId": "12f6aa57-a8d0-4beb-fa66-ae4c1fde5707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Relu -> Sigmoid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.98      0.96        55\n",
      "      benign       0.99      0.97      0.98        88\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.97      0.97      0.97       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Testing Neural Network\n",
    "'''\n",
    "y_predicted = nn.predict(x_test) # predict \n",
    "\n",
    "#plot\n",
    "print(f\"Classification Report for {nn.neural_net_type}\\n\\n\")\n",
    "print(classification_report(y_test, y_predicted, target_names=dataset.target_names))\n",
    "plot_confusion_matrix(nn, x_test, y_test, cmap=plt.cm.Blues, display_labels=dataset.target_names)  \n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "2BiCt8H1m86M",
    "outputId": "3877951d-85b3-4a6c-a876-f01f90381154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.870830\n",
      "Loss after iteration 2000: 0.007897\n",
      "Loss after iteration 4000: 0.000412\n",
      "Loss after iteration 6000: 0.000013\n",
      "Loss after iteration 8000: 0.000026\n",
      "Loss after iteration 10000: 0.000455\n",
      "Loss after iteration 12000: 0.028189\n",
      "Loss after iteration 14000: 0.000147\n",
      "Loss after iteration 16000: 0.000014\n",
      "Loss after iteration 18000: 0.000014\n",
      "Loss after iteration 20000: 0.000005\n",
      "Loss after iteration 22000: 0.000234\n",
      "Loss after iteration 24000: 0.009857\n",
      "Loss after iteration 26000: 0.001514\n",
      "Loss after iteration 28000: 0.000003\n",
      "Loss after iteration 30000: 0.000485\n",
      "Loss after iteration 32000: 0.007948\n",
      "Loss after iteration 34000: 0.000000\n",
      "Loss after iteration 36000: 0.000001\n",
      "Loss after iteration 38000: 0.252251\n",
      "Loss after iteration 40000: 0.000001\n",
      "Loss after iteration 42000: 0.000000\n",
      "Loss after iteration 44000: 0.002388\n",
      "Loss after iteration 46000: 0.007836\n",
      "Loss after iteration 48000: 0.000000\n",
      "Loss after iteration 50000: 0.000000\n",
      "Loss after iteration 52000: 0.000670\n",
      "Loss after iteration 54000: 0.000000\n",
      "Loss after iteration 56000: 0.000509\n",
      "Loss after iteration 58000: 0.001006\n",
      "Loss after iteration 60000: 0.000000\n",
      "Loss after iteration 62000: 0.000002\n",
      "Loss after iteration 64000: 0.003064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Training the Neural Network, you do not need to modify this cell\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "'''\n",
    "dataset = load_breast_cancer() # load the dataset\n",
    "x, y = dataset.data, dataset.target\n",
    "x = MinMaxScaler().fit_transform(x) #normalize data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1) #split data\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.reshape(1,-1), y_test #condition data\n",
    "\n",
    "nn = dlnet(x_train,y_train,lr=0.1) # initalize neural net class\n",
    "nn.stochastic_gradient_descent(x_train, y_train, iter = 66000) #train\n",
    "\n",
    "# create figure\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f'Training: {nn.neural_net_type}')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Tanh -> Sigmoid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.96      0.95        55\n",
      "      benign       0.98      0.97      0.97        88\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Testing Neural Network\n",
    "'''\n",
    "y_predicted = nn.predict(x_test) # predict \n",
    "\n",
    "#plot\n",
    "print(f\"Classification Report for {nn.neural_net_type}\\n\\n\")\n",
    "print(classification_report(y_test, y_predicted, target_names=dataset.target_names))\n",
    "plot_confusion_matrix(nn, x_test, y_test, cmap=plt.cm.Blues, display_labels=dataset.target_names)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "## 2:  (Bounus for all) Image Classification based on Convolutional Neural Networks [15pts] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAcYa40pm86P"
   },
   "source": [
    "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research. In this part, you will build a convolutional neural network based on Keras to solve the image classification task for MINIST. If you haven't installed TensorFlow, you can install the package by **pip** command or train your model by uploading HW4 notebook to [Colab](https://colab.research.google.com/) directly. Colab contains all packages you need for this section.  \n",
    "\n",
    "Hint1: [First contact with Keras](https://keras.io/about/)\n",
    "\n",
    "Hint2: [How to Install Keras](https://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/)\n",
    "\n",
    "Hint3[CS231n Tutorial (Layers used to build ConvNets) ](https://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSeZNmCm86P"
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5wmr3mt2yOu"
   },
   "source": [
    "### Load MINIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxDuP6Yq2yOv"
   },
   "source": [
    "We use [MINIST](http://yann.lecun.com/exdb/mnist/) dataset to train our model. MINIST is a subset of a larger set available from NIST. MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. Each example is $28\\times 28$ pixel grayscale image of handwritten digits between 0 to 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv8fRYkp2yOv",
    "outputId": "a8884fe7-a4f6-42e8-dfa7-b86b83c76349"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# split data between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "#set num of classes\n",
    "num_classes = 10\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IropaXmi2yOy"
   },
   "source": [
    "### Load some images from MINIST##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaUQz7x52yOy",
    "outputId": "fe6cb811-a827-4f91-b89f-6b14b416d8f8"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# Show some images from MINIST\n",
    "for i in range(0,25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    image=x_train[i].reshape((28,28))\n",
    "    plt.imshow(image,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJEjYyism86Y"
   },
   "source": [
    "As you can see from above, the MINIST dataset contains handwritten digits from 0 to 9. The digits have been size-normalized and centered in fixed-size images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deWtvYFKm86Z"
   },
   "source": [
    "### Build convolutional neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8ioOgl1m86Z"
   },
   "source": [
    "In this part, you need to build a convolutional neural network that contains 2 convolutional layers. The architecture of thie model is:\n",
    "\n",
    " **[INPUT - CONV - RELU - MAXPOOL - CONV - RELU - MAXPOOL - FC1 - FC2]** [1]\n",
    "\n",
    "\n",
    "> INPUT: [$28\\times28\\times1$] will hold the raw pixel values of the image, in this case, an image of width 28, height 28, and with only one color channels.\n",
    "\n",
    "\n",
    "> CONV: Conv. layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to the input volume. We decide to set the kernel_size $3\\times3$ for the both Conv. layers. For example, the output of the Conv. layer may like $[26\\times26\\times32]$ if we use 32 filters. \n",
    "\n",
    "\n",
    "> RELU: As we mentioned in the previous section, the Relu layer will apply an elementwise activation function, such as the max(0,x) thresholding at zero, which leaves the size of the volume unchanged.\n",
    "\n",
    "> MAXPOOL: MAXPOOL layer will perform a downsampling operation along the spatial dimensions (width, hight). \n",
    "\n",
    "> FC1: First Fully-Connected layer, we use **ReLu** as the activation function. The dimension of the output space is 128.\n",
    "\n",
    "> FC2: Second Fully-Connected layer will compute the class scores. We use **Softmax** as the activation function. The dimension of the output space is the number of class. \n",
    "\n",
    "**Loss function**: Crossentropy (mentioned in the previous section) \n",
    "\n",
    "**optimizer**: Stochastic gradient descent(SGD)\n",
    "\n",
    "\n",
    "\n",
    "[1] CS231n: https://cs231n.github.io/convolutional-networks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6fLE8Wpm86Z",
    "outputId": "e3bde7b8-cf2f-4f25-bb8b-2d8adcf4f9b6"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# Show the architecture of the model\n",
    "achi=plt.imread('Architecture.png')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(achi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FagsUubm86c"
   },
   "source": [
    "#### Defining Variables ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZp2AB_r2yO0"
   },
   "outputs": [],
   "source": [
    "# Defining Variables\n",
    "# Do not change the value of num_classes. \n",
    "# You can adjust of adding parameters to train your model\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "lr= 1e-3 #learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F5PWx17m86g"
   },
   "source": [
    "#### Defining model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP4IaJPP2yO3"
   },
   "outputs": [],
   "source": [
    "def create_net():\n",
    "  '''\n",
    "  In this function you are going to build a convolutional neural network based on Keras.\n",
    "  First, use Sequential() to set the inference features on this model. \n",
    "  Then, use model.add() and model.compile() to build your own model\n",
    "  Return: model you build\n",
    "  '''\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKpTMi2rm86i",
    "outputId": "0082a080-9992-4b04-d721-ca0eb29f074a"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# model.summary() gives you details of your architecture.\n",
    "#You can compare your architecture with the 'Architecture.png'\n",
    "model=create_net()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85m3oJ-1Voxe"
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYHAZAnZm86l"
   },
   "source": [
    "**Tuning:** Training the network is the next thing to try.  You can set your parameter at the **Defining Variable** section. If your parameters are set properly, you should see the loss of the validation set decreased and the value of accuracy increased. It may take more than 20 minutes to train your model. \n",
    "\n",
    "**Expected Result:** You should be able to achieve more than $90\\%$ accuracy on the test set to get full 15 points. If you achieve accuracy between $80\\%$ to $90\\%$, you will only get half points of this part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUSAXVGPm86l"
   },
   "source": [
    "#### Train your own CNN model####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCpEenFbm86m",
    "outputId": "fbbae3e9-c9b5-4dd0-ba15-b095678e07a0"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0dD-7Edm86o",
    "outputId": "dd9af087-5681-4d2c-f080-31bad8ed4c43"
   },
   "outputs": [],
   "source": [
    "# Helper function, You don't need to modify it\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summary\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FxKE3QN2yO6"
   },
   "source": [
    "## 3: Random Forests [40pts] <span style=\"color:blue\">**[P]**</span> <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "**NOTE**: Please use sklearn's DecisionTreeClassifier in your Random Forest implementation.\n",
    "[You can find more details about this classifier here.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zP6DnH62yO9"
   },
   "source": [
    "### 3.1 Random Forest Implementation (30 pts) <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of a decision tree, we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner. This is commonly called a Random Forest.\n",
    "\n",
    "We can build a Random Forest as a collection of decision trees, as follows:\n",
    "\n",
    "1) For every tree in the random forest, we're going to \n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in a), choose attributes at random to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (70% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen to a certain depth.\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In RandomForest Class, \n",
    "1. X is assumed to be a matrix with num_training rows and num_features columns where num_training is the\n",
    "number of total records and num_features is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length num_training.\n",
    "\n",
    "**NOTE:** Lookout for TODOs for the parts that needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMCsVv7U2yO7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=50, max_depth=None, max_features=0.7):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initialization done here\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstraps_row_indices = []\n",
    "        self.feature_indices = []\n",
    "        self.out_of_bag = []\n",
    "        self.decision_trees = [sklearn.tree.DecisionTreeClassifier(max_depth=max_depth, criterion='entropy') for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features, random_seed = None):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        - Randomly select indices of size num_training with replacement corresponding to row locations of \n",
    "          selected samples in the original dataset.\n",
    "        - Randomly select indices without replacement corresponding the column locations of selected features in the original feature\n",
    "           list (num_features denotes the total number of features in the training set, max_features denotes the percentage \n",
    "           of features that are used to fit each decision tree).\n",
    "        \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \n",
    "        Args: \n",
    "        - num_training: an integer N representing the total number of training instances.\n",
    "        - num_features: an integer D representing the total number of features.\n",
    "            \n",
    "        Returns:\n",
    "        - row_idx: (N,) numpy array of row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: 1-D array of column indices corresponding to the column locations of selected features in the original feature list. \n",
    "                    \n",
    "        Hint: Consider using np.random.choice.\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "            \n",
    "    def bootstrapping(self, num_training, num_features):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "        - num_training: an integer N representing the total number of training instances.\n",
    "        - num_features: an integer D representing the total number of features.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training)))\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
    "            total = total - set(row_idx)\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        Note that you need to use the row indices and column indices.\n",
    "        \n",
    "        Args:\n",
    "        -X: NxD numpy array, where N is number \n",
    "           of instances and D is the dimensionality of each \n",
    "           instance\n",
    "        -y: Nx1 numpy array, the predicted labels\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "    \n",
    "    def OOB_score(self, X, y):\n",
    "        # helper function. You don't have to modify it\n",
    "        # This function computes the accuracy of the random forest model predicting y given x.\n",
    "        accuracy = []\n",
    "        for i in range(len(X)):\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators):\n",
    "                if i in self.out_of_bag[t]:\n",
    "                    predictions.append(self.decision_trees[t].predict(np.reshape(X[i][self.feature_indices[t]], (1,-1)))[0])\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8E74hGIm86t"
   },
   "source": [
    "### 3.2 Hyperparameter Tuning with a Random Forest (5pts) <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In machine learning, hyperparameters are parameters that are set before the learning process begins. The max_depth, num_estimators, or max_features variables from 3.1 are examples of different hyperparameters for a random forest model. In this section, you will tune your random forest model on an e-commerce dataset to achieve a high accuracy on a classifying revenue sessions (whether a customer will purchase a product) from user behavior.\n",
    "\n",
    "Let's first review the dataset in a bit more detail.\n",
    "\n",
    "#### Dataset Objective\n",
    "\n",
    "Imagine that we are the founders of a new e-commerce company that uses machine learning to optimize the user experience. We are tasked with the responsibility of coming up with a method for determining the likelihood of a shopping session ending in a purchase being made. We will then use this information to adjust pricing and services to encourage more purchasing.\n",
    "\n",
    "After much deliberation amongst the team, you come to a conclusion that we can use past online shopping data to predict the future occurence of revenue sessions. \n",
    "\n",
    "We will use our random forest algorithm from Q3.1 to predict if a shopping session ends in a purchase.\n",
    "\n",
    "You can find more information on the dataset [here](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYwbegdZm86t"
   },
   "source": [
    "#### Loading the dataset\n",
    "\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "1. Administrative : continuous variable\n",
    "2. Administrative_Duration\t: continuous variable\n",
    "3. Informational : continuous variable\n",
    "4. Informational_Duration : continuous variable\n",
    "5. ProductRelated : continuous variable\n",
    "6. ProductRelated_Duration : continuous variable\n",
    "7. BounceRates : continuous variable\n",
    "8. ExitRates : continuous variable\n",
    "9. PageValues : continuous variable\n",
    "10. SpecialDay : continuous variable\n",
    "11. Month\t: categorical variable\n",
    "12. OperatingSystems\t: continuous variable\n",
    "13. Browser : continuous variable\n",
    "14. Region : continuous variable\n",
    "14. TrafficType : continuous variable\n",
    "14. VisitorType : categorical variable\n",
    "14. Weekend : continuous variable\n",
    "14. Revenue : target variable -------------> **Your random forest model will try to predict this variable. A \"True\" value in this column means a shopper purchased an item given their user behavior described by features 1-17, while a \"False\" label indicates that a shopper did not purchase an item.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01NrkRJ3SbRS"
   },
   "outputs": [],
   "source": [
    "# Logic for loading in datasets. DO NOT MODIFY anything in this block.\n",
    "from sklearn import preprocessing\n",
    "preprocessor = preprocessing.LabelEncoder()\n",
    "\n",
    "data_train = pd.read_csv(\"datasets/hw4_fall2020_data_train.csv\")\n",
    "data_test = pd.read_csv(\"datasets/hw4_fall2020_data_test.csv\")\n",
    "\n",
    "X_train = data_train.drop(columns = 'Revenue')\n",
    "y_train = data_train['Revenue']\n",
    "X_test = np.array(data_test.drop(columns = 'Revenue'))\n",
    "y_test = np.array(data_test['Revenue'])\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "         \n",
    "#The following lines of code converts columns holding categorical or boolean variables into integers.\n",
    "X_train[:,10] = preprocessor.fit_transform(X_train[:,10])\n",
    "X_test[:,10] = preprocessor.fit_transform(X_test[:,10])\n",
    "X_train[:,-2] = preprocessor.fit_transform(X_train[:,-2])\n",
    "X_test[:,-2] = preprocessor.fit_transform(X_test[:,-2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlxXUpNE2yPA"
   },
   "source": [
    "In the following codeblock, train your random forest model with different values for max_depth, n_estimators, or max_features and evaluate each model on the held-out test set. Try to choose a combination of hyperparameters that maximizes your prediction accuracy on the test set (aim for 92%+). **Once you are satisfied with your chosen parameters, change the default values for max_depth, n_estimators, and max_features in the __init__ function of your RandomForest class in random_forest.py to your chosen values, and then submit this file to Gradescope. You must achieve at least a 92% accuracy against a hidden test set (this will NOT the same as the test set provided here) in Gradescope to receive full credit for this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "6n8GGVU7tYGh",
    "outputId": "4a83b962-d917-4a53-9dc8-2681735d9396"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many decision trees are fitted for the random forest.\n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each decision tree.\n",
    "\n",
    "Tune these three parameters to achieve a better accuracy. While you can use the provided test set to \n",
    "evaluate your implementation, you will need to obtain 92% on a hidden test set to receive full credit \n",
    "for this section.\n",
    "\"\"\"\n",
    "import sklearn.ensemble\n",
    "n_estimators = 1 #Hint: Consider values between 5-12.\n",
    "max_depth = 1 # Hint: Consider values betweeen 3-12\n",
    "max_features = 0.5 # Hint: Consider values betweeen 0.7-1.0.\n",
    "\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "    \n",
    "accuracy=random_forest.OOB_score(X_test, y_test)\n",
    "    \n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ3Urx3Em86y"
   },
   "source": [
    "### 3.3 Plotting Feature Importance (5pts) <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "While building tree-based models, it's common to quantify how well splitting on a particular feature in a decision tree helps with predicting the target label in a dataset. Machine learning practicioners typically use \"Gini importance\", or the (normalized) total reduction in entropy brought by that feature to evaluate how important that feature is for predicting the target variable. \n",
    "\n",
    "Gini importance is typically calculated as the reduction in entropy from reaching a split in a decision tree weighted by the probability of reaching that split in the decision tree. Sklearn internally computes the probability for reaching a split by finding the total number of samples that reaches it during the training phase divided by the total number of samples in the dataset. This weighted value is our feature importance.\n",
    "\n",
    "Let's think about what this metric means with an example. A high probabiity of reaching a split on \"TrafficType\" in a decision tree trained on our e-commerce dataset (many samples will reach this split for a decision) and a large reduction in entropy from splitting on \"TrafficType\" will result in a high feature importance value for \"TrafficType\". This could mean \"TrafficType\" is a very important feature for predicting a customer's revenue session. On the other hand, a low probability of reaching a split on \"Month\" in a decision tree (few samples will reach this split for a decision) and a low reduction in entropy from splitting on \"Month\" will result in a low feature importance value. This could mean \"Month\" is not a very informative feature for predicting the revenue session in our decision tree. **Thus, the higher the feature importance value, the more important the feature is to predicting the target label.**\n",
    "\n",
    "Fortunately for us, fitting a sklearn.DecisionTreeClassifier to a dataset auomatically computes the Gini importance for every feature in the decision tree and stores these values in a **feature_importances_** variable. [Review the docs for more details on how to access this variable](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)\n",
    "\n",
    "In the function below, display a bar plot that shows the feature importance values for at least one decision tree in your tuned random forest from Q3.2, and briefly comment on whether any features have noticeably higher / or lower importance weights than others. [Note that there isn't a \"correct\" answer here. We simply want you to investigate how different features in your random forest contribute to predicting the target variable].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "irV3hL6mm86z",
    "outputId": "30612200-e1ff-4c2a-c367-bf643244b0cb"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(random_forest):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    -Display a bar plot showing the feature importance of every feature in \n",
    "     at least one decision tree from the tuned random_forest from Q3.2.\n",
    "     \n",
    "    Args:\n",
    "        random_forest: This is your implemented and tuned random forest from Q3.2. \n",
    "    Returns:\n",
    "        None. Calling this function should simply display the aforementioned feature importance bar chart\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "plot_feature_importance(random_forest)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1udwVq0PVFz"
   },
   "source": [
    "## 4: SVM (30 Pts) <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG6jrvc_m861"
   },
   "source": [
    "### 4.1 Fitting an SVM classifier by hand (20 Pts) <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Consider a dataset with 2 points in 1-dimensional space: $(x_1 = -3, y_1 = 1)$ and $(x_2 = 2, y_2 = 1)$. Here $x$ are the point coordinates and $y$ are the classes.\n",
    "\n",
    "Consider mapping each point to 3-dimensional space using the feature vector $\\phi(x) = [1,2x, x^2]$. (This is equivalent to using a second order polynomial kernel.) The max margin classifier has the form\n",
    "\n",
    "$$min ||\\mathbf{\\theta}||^2 s.t.$$\n",
    "\n",
    "$$y_1(\\phi(x_1)\\mathbf{\\theta} + b)  1 $$\n",
    "\n",
    "$$y_2(\\phi(x_2)\\mathbf{\\theta}+ b)  1 $$\n",
    "\n",
    "**Hint:** $\\phi(x_1)$ and $\\phi(x_2)$ are the suppport vectors. We have already given you the solution for the suppport vectors and you need to calculate back the parameters. Margin is equal to $\\frac{1}{||\\mathbf{\\theta}||}$ and full margin is equal to $\\frac{2}{||\\mathbf{\\theta}||}$.\n",
    "\n",
    "(1) Find a vector parallel to the optimal vector $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "(2) Calculate the value of the margin achieved by this $\\mathbf{\\theta}$? (4pts)\n",
    "\n",
    "(3) Solve for $\\mathbf{\\theta}$, given that the margin is equal to $1/||\\mathbf{\\theta}||$. (4pts)\n",
    "\n",
    "(4) Solve for $b$ using your value for $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "(5) Write down the form of the discriminant function $f(x) = \\phi(x)\\mathbf{\\theta}+b$ as an explicit function of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NacpSq-Tm862"
   },
   "source": [
    "### 4.2 Feature Mapping (10 Pts) <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Let's look at a dataset where the datapoint can't be classified with a good accuracy using a linear classifier. Run the below cell to generate the dataset.\n",
    "\n",
    "We will also see what happens when we try to fit a linear classifier to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "LmyYQFamm862",
    "outputId": "2ab03801-ca5f-4f98-ffdf-a1ba1aa38f39"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Generate dataset\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=.05)\n",
    "\n",
    "y = np.where(y == 0, -1, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, marker = '.') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaVoa1J-m865"
   },
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(X, y, feature_new=None, h=0.02):\n",
    "    '''\n",
    "    You don't have to modify this function\n",
    "    \n",
    "    Function to vizualize decision boundary\n",
    "    \n",
    "    feature_new is a function to get X with additional features\n",
    "    '''\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx_1, xx_2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                         np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    if X.shape[1] == 2:\n",
    "        Z = svm_cls.predict(np.c_[xx_1.ravel(), xx_2.ravel()])\n",
    "    else:\n",
    "        X_conc = np.c_[xx_1.ravel(), xx_2.ravel()]\n",
    "        X_new = feature_new(X_conc)\n",
    "        Z = svm_cls.predict(X_new)\n",
    "\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "    plt.contourf(xx_1, xx_2, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.xlim(xx_1.min(), xx_1.max())\n",
    "    plt.ylim(xx_2.min(), xx_2.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "ZJfVwgE2m867",
    "outputId": "7f5cf1c7-bc5e-49d9-e8a4-1a38145d8b11"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Try to fit a linear classifier to the dataset\n",
    "\n",
    "svm_cls = svm.LinearSVC()\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, \n",
    "                                                           y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuShu_ECm869"
   },
   "source": [
    "We can see that we need a non-linear boundary to be able to successfully classify data in this dataset. By mapping the current feature x to a higher space with more features, linear SVM could be performed on the features in the higher space to learn a non-linear decision boundary. In the function below add additional features which can help classify in the above dataset. After creating the additional features use code in the further cells to see how well the features perform on the test set.  \n",
    "(**Hint:** Think of the shape of the decision boundary that would best separate the above points. What additional features could help map the linear boundary to the non-linear one? Look at [this](https://xavierbourretsicotte.github.io/Kernel_feature_map.html) for a detailed analysis of doing the same for points separable with a circular boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6VPMhbjm869"
   },
   "outputs": [],
   "source": [
    "def create_nl_feature(X):\n",
    "    '''\n",
    "    TODO - Create additional features and add it to the dataset\n",
    "    \n",
    "    returns:\n",
    "        X_new - (N, d + num_new_features) array with \n",
    "                additional features added to X such that it\n",
    "                can classify the points in the dataset.\n",
    "    '''\n",
    "    #  Delete this line when you implement the function\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP0ILDgFm86_"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Create new features\n",
    "\n",
    "X_new = create_nl_feature(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "VaUahNX9m87B",
    "outputId": "c4624aea-2dd0-41ad-e844-1ecfdb1e8de9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Fit to the new features and vizualize the decision boundary\n",
    "# You should get more than 90% accuracy on test set\n",
    "\n",
    "svm_cls = svm.LinearSVC()\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train, create_nl_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsGzl0qdnrTD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_FALL2020_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
